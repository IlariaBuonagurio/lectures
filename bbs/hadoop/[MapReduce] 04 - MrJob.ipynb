{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A more pythonic MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With Hadoop Streaming we switched MapReduce from Java to Python.\n",
    "\n",
    "How could we improve some more? \n",
    "\n",
    "What problems do we deal with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have to directly deal with HDFS\n",
    "* move input files\n",
    "* recover outputs\n",
    "* human make mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not easy debugging\n",
    "\n",
    "* logs via jobtracker\n",
    "* errors are Java stacktrace, often unrelated with the real problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have to write two different files\n",
    "* one for the mapper\n",
    "* one for the reducer\n",
    "* not a **module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is there a Python part of the language that can help representing a MapReduce task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A python class\n",
    "\n",
    "class myclass(object):\n",
    "    \n",
    "    a_property = 42\n",
    "    \n",
    "    def a_method(self):\n",
    "        pass\n",
    "    def another_method(self, var):\n",
    "        print(var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create instance of our class\n",
    "instance = myclass()\n",
    "# Use it\n",
    "instance.a_method()\n",
    "instance.another_method(\"test\")\n",
    "instance.a_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MapReduce(object):\n",
    "    \"\"\" A MapReduce class prototype \"\"\"\n",
    "    \n",
    "    def mapper(self, line):\n",
    "        pass\n",
    "    def reducer(self, sorted_line):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MRJob \n",
    "A more pythonic MapReduce library from Yelp\n",
    "\n",
    "<img src='https://avatars1.githubusercontent.com/u/49071?v=3&s=400' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> “Easiest route to Python programs that run on Hadoop”\n",
    "\n",
    "Install with: \n",
    "```bash\n",
    "pip install mrjob\n",
    "```\n",
    "\n",
    "**Running modes**\n",
    "* Test your code locally without installing Hadoop \n",
    "* or run it on a cluster of your choice!\n",
    "    - Integrates with Amazon **Elastic MapReduce** (EMR)\n",
    "    - same code with local, Hadoop, EMR\n",
    "    - easy to run your job in the cloud as on your laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does MrJob work?\n",
    "\n",
    "* Python module built **on top of Hadoop Streaming**\n",
    "    - HS jar opens a subprocess to your code\n",
    "    - sends it input via stdin\n",
    "    - gathers results via stdout\n",
    "* Wrap HDFS pre and post processing if hadoop exists\n",
    "* a consistent interface across every environment it supports\n",
    "* automatically serializes/deserializes data flow out of each task \n",
    "    - e.g. JSON: json.loads() and json.dumps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting hands dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A job is defined by a class extended from MRJob package\n",
    "\n",
    "* Contains methods that define the steps of a Hadoop job\n",
    "* A “step” consists of a mapper, a combiner, and a reducer. \n",
    "* All of those  are optional, though you must have at least one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class myjob(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        pass\n",
    "    def combiner(self, key, values):\n",
    "        pass\n",
    "    def reducer(self, key, values):\n",
    "        pass\n",
    "    def steps(self):\n",
    "        return [ MRStep(mapper=self.mapper, combiner=self.combiner, reducer=self.reducer) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mapper\n",
    "\n",
    "The mapper() method takes a key and a value as args\n",
    "\n",
    "```\n",
    "    def mapper(self, _, line):\n",
    "        pass\n",
    "```\n",
    "\n",
    "* E.g. key is ignored and a single line of text input is the value\n",
    "\n",
    "* Yields as many key-value pairs as it likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Yield?\n",
    "\n",
    "**Warning**: `yield` != `return`\n",
    "\n",
    "> yield return a generator, the one you usually use with  ```print i; for i in generator```\n",
    "    \n",
    "Example\n",
    "```python\n",
    "def mygen():\n",
    "    for i in range(1,10):\n",
    "    # THIS IS WHAT HAPPENS INSIDE THE MAPPER/REDUCER\n",
    "        yield i, “value” \n",
    "\n",
    "for key, value in mygen():\n",
    "    print key, value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reducer\n",
    "\n",
    "The reduce() method takes a key and an iterator of values\n",
    "\n",
    "```\n",
    "    def reducer(self, key, values):\n",
    "        pass\n",
    "```\n",
    "\n",
    "* Also yields as many key-value pairs as it likes\n",
    "    * E.g. it sums the values for each key\n",
    "* Represent the  numbers of characters, words, and lines in the initial input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's write our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: mydir=mymrjob\n",
      "env: myinput=/data/lectures/data/books/twolines.txt\n",
      "env: myscript=mymrjob/wordcount.py\n",
      "env: myoutput=mymrjob/out.txt\n",
      "env: mylog=mymrjob/out.log\n"
     ]
    }
   ],
   "source": [
    "# Little configuration\n",
    "\n",
    "mydir = \"mymrjob\"\n",
    "%env mydir = $mydir\n",
    "myinput = \"/data/lectures/data/books/twolines.txt\"\n",
    "%env myinput $myinput\n",
    "myscript = mydir + \"/wordcount.py\"\n",
    "%env myscript $myscript\n",
    "\n",
    "%system mkdir -p $mydir\n",
    "%env myoutput $mydir/out.txt\n",
    "%env mylog $mydir/out.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Create the job file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymrjob/wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $myscript\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "class MRWordCount(MRJob):\n",
    "    \"\"\" Wordcount with MapReduce in a pythonic way\"\"\"\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split(' '):\n",
    "             yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note!\n",
    "    \n",
    "Thanks to MrJob and generators we do not care inside the reducer to check when the value is changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I/O\n",
    "\n",
    "You can pass input via stdin but be aware that mrjob will just dump it to a file first:\n",
    "```bash\n",
    "$ python my_job.py < input.txt\n",
    "```\n",
    "\n",
    "You can pass multiple input files, mixed with stdin (using the – character)\n",
    "```bash\n",
    "$ python my_job.py input1.txt input2.txt - < input3.txt\n",
    "```\n",
    "\n",
    "By default, output will be written to stdout.\n",
    "```bash\n",
    "$ python my_job.py input.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute MrJob\n",
    "! python $myscript $myinput 1> $myoutput 2> $mylog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bye\"\t1\r\n",
      "\"goodbye\"\t1\r\n",
      "\"hadoop\"\t2\r\n",
      "\"hello\"\t2\r\n",
      "\"world\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "%cat $myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\r\n",
      "Creating temp directory /tmp/wordcount.jovyan.20160315.231000.353160\r\n",
      "Running step 1 of 1...\r\n",
      "Streaming final output from /tmp/wordcount.jovyan.20160315.231000.353160/output...\r\n",
      "Removing temp directory /tmp/wordcount.jovyan.20160315.231000.353160...\r\n"
     ]
    }
   ],
   "source": [
    "%cat $mylog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's an empty **template** to work with in copy/paste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" MapReduce easily with Python \"\"\"\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class job(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        pass\n",
    "    def reducer(self, key, line):\n",
    "        pass\n",
    "    def steps(self):\n",
    "        return [ \n",
    "            MRStep(mapper=self.mapper, reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    job.run()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With MRStep you define iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Convert your exercise for vowels inside the divine comedy into MrJob class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By default, mrjob will run your job on your local (normal) environment) in a single Python process \n",
    "\n",
    "You change the way the job is run with the `-r/--runner` option:\n",
    "\n",
    "```\n",
    "-r inline, -r local, -r hadoop, or -r emr\n",
    "```\n",
    "\n",
    "Use also `--verbose` option to show all the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we just need to add `-r hadoop`.\n",
    "\n",
    "<small>Note: The `capture` *magic* is another way we could handle output.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%capture hadoop_out\n",
    "# Execute MrJob on Hadoop and let the magic handle outputs\n",
    "! python $myscript $myinput -r hadoop 2> $mylog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bye\"\t1\r\n",
      "\"goodbye\"\t1\r\n",
      "\"hadoop\"\t2\r\n",
      "\"hello\"\t2\r\n",
      "\"world\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "hadoop_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\r\n",
      "Using Hadoop version 2.6.0\r\n",
      "Copying local files to hdfs:///user/jovyan/tmp/mrjob/wordcount.jovyan.20160315.231008.131608/files/...\r\n",
      "Running step 1 of 1...\r\n",
      "  packageJobJar: [/tmp/hadoop-unjar8386463698946503097/] [] /tmp/streamjob581799815579415374.jar tmpDir=null\r\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\r\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\r\n",
      "  Total input paths to process : 1\r\n",
      "  number of splits:2\r\n",
      "  Submitting tokens for job: job_1458083121911_0001\r\n",
      "  Submitted application application_1458083121911_0001\r\n",
      "  The url to track the job: http://76752dc90450:8088/proxy/application_1458083121911_0001/\r\n",
      "  Running job: job_1458083121911_0001\r\n",
      "  Job job_1458083121911_0001 running in uber mode : false\r\n",
      "   map 0% reduce 0%\r\n",
      "   map 100% reduce 0%\r\n",
      "   map 100% reduce 100%\r\n",
      "  Job job_1458083121911_0001 completed successfully\r\n",
      "  Output directory: hdfs:///user/jovyan/tmp/mrjob/wordcount.jovyan.20160315.231008.131608/output\r\n",
      "Counters: 49\r\n",
      "\tFile Input Format Counters \r\n",
      "\t\tBytes Read=74\r\n",
      "\tFile Output Format Counters \r\n",
      "\t\tBytes Written=51\r\n",
      "\tFile System Counters\r\n",
      "\t\tFILE: Number of bytes read=104\r\n",
      "\t\tFILE: Number of bytes written=329064\r\n",
      "\t\tFILE: Number of large read operations=0\r\n",
      "\t\tFILE: Number of read operations=0\r\n",
      "\t\tFILE: Number of write operations=0\r\n",
      "\t\tHDFS: Number of bytes read=388\r\n",
      "\t\tHDFS: Number of bytes written=51\r\n",
      "\t\tHDFS: Number of large read operations=0\r\n",
      "\t\tHDFS: Number of read operations=9\r\n",
      "\t\tHDFS: Number of write operations=2\r\n",
      "\tJob Counters \r\n",
      "\t\tData-local map tasks=2\r\n",
      "\t\tLaunched map tasks=2\r\n",
      "\t\tLaunched reduce tasks=1\r\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12473344\r\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4098048\r\n",
      "\t\tTotal time spent by all map tasks (ms)=12181\r\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12181\r\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4002\r\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4002\r\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12181\r\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4002\r\n",
      "\tMap-Reduce Framework\r\n",
      "\t\tCPU time spent (ms)=2130\r\n",
      "\t\tCombine input records=0\r\n",
      "\t\tCombine output records=0\r\n",
      "\t\tFailed Shuffles=0\r\n",
      "\t\tGC time elapsed (ms)=166\r\n",
      "\t\tInput split bytes=314\r\n",
      "\t\tMap input records=2\r\n",
      "\t\tMap output bytes=82\r\n",
      "\t\tMap output materialized bytes=110\r\n",
      "\t\tMap output records=8\r\n",
      "\t\tMerged Map outputs=2\r\n",
      "\t\tPhysical memory (bytes) snapshot=889561088\r\n",
      "\t\tReduce input groups=5\r\n",
      "\t\tReduce input records=8\r\n",
      "\t\tReduce output records=5\r\n",
      "\t\tReduce shuffle bytes=110\r\n",
      "\t\tShuffled Maps =2\r\n",
      "\t\tSpilled Records=16\r\n",
      "\t\tTotal committed heap usage (bytes)=530579456\r\n",
      "\t\tVirtual memory (bytes) snapshot=2645000192\r\n",
      "\tShuffle Errors\r\n",
      "\t\tBAD_ID=0\r\n",
      "\t\tCONNECTION=0\r\n",
      "\t\tIO_ERROR=0\r\n",
      "\t\tWRONG_LENGTH=0\r\n",
      "\t\tWRONG_MAP=0\r\n",
      "\t\tWRONG_REDUCE=0\r\n",
      "Streaming final output from hdfs:///user/jovyan/tmp/mrjob/wordcount.jovyan.20160315.231008.131608/output...\r\n",
      "Removing HDFS temp directory hdfs:///user/jovyan/tmp/mrjob/wordcount.jovyan.20160315.231008.131608...\r\n"
     ]
    }
   ],
   "source": [
    "%cat $mylog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Protocols\n",
    "\n",
    "http://mrjob.readthedocs.org/en/latest/guides/writing-mrjobs.html#protocols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MRJob add many comodities!\n",
    "\n",
    "Some of them may result expensive on heavy computing.\n",
    "\n",
    "For example MapReduce data transfer is serialize in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class MyMRJob(mrjob.job.MRJob):\n",
    "\n",
    "    # these are the defaults\n",
    "    INPUT_PROTOCOL = mrjob.protocol.RawValueProtocol\n",
    "    INTERNAL_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    OUTPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see what it means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing protocol.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile protocol.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split(' '):\n",
    "             yield word.lower(), 1\n",
    "\n",
    "                \n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bye\"\t1\r\n",
      "\"goodbye\"\t1\r\n",
      "\"hadoop\"\t2\r\n",
      "\"hello\"\t2\r\n",
      "\"world\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "! python protocol.py /data/lectures/data/books/twolines.txt 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting protocol.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile protocol.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import PickleProtocol\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "\n",
    "    # Optimization on internal protocols\n",
    "    INTERNAL_PROTOCOL = PickleProtocol\n",
    "    OUTPUT_PROTOCOL = PickleProtocol\n",
    "    \n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split(' '):\n",
    "             yield word.lower(), 1\n",
    "\n",
    "                \n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\x80\\x03X\\x03\\x00\\x00\\x00byeq\\x00.\t\\x80\\x03K\\x01.\r\n",
      "\\x80\\x03X\\x05\\x00\\x00\\x00helloq\\x00.\t\\x80\\x03K\\x02.\r\n",
      "\\x80\\x03X\\x05\\x00\\x00\\x00worldq\\x00.\t\\x80\\x03K\\x02.\r\n",
      "\\x80\\x03X\\x06\\x00\\x00\\x00hadoopq\\x00.\t\\x80\\x03K\\x02.\r\n",
      "\\x80\\x03X\\x07\\x00\\x00\\x00goodbyeq\\x00.\t\\x80\\x03K\\x01.\r\n"
     ]
    }
   ],
   "source": [
    "! python protocol.py /data/lectures/data/books/twolines.txt 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting deeper\n",
    "\n",
    "Let's try this together step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "myfile = \"/data/lectures/data/books/prince.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How does the wordcounter work with this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile job.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SomeJob(MRJob):\n",
    "    \"\"\" Counting the words \"\"\"\n",
    "    \n",
    "    def mapper(self, key, line):\n",
    "        # Removing extra characters\n",
    "        for word in line.strip('.;,()').split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    SomeJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "! python job.py $myfile 1> out.txt 2> err.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_output(outfile):\n",
    "    \"\"\" Get Hadoop results from MrJob output file \"\"\"\n",
    "    \n",
    "    data = {}\n",
    "\n",
    "    if os.path.exists(outfile):\n",
    "        with open(outfile) as out:\n",
    "            outstring = out.read()\n",
    "        for line in outstring.split('\\n'):\n",
    "            if line.strip() == '':\n",
    "                continue\n",
    "            tmp = line.split('\\t')\n",
    "            data[tmp[0].strip('\"')] = int(tmp[1])\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Recover last cell output\n",
    "data = get_output(\"out.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def slicedict(mydict, num):\n",
    "    \"\"\" Get maximum 'num' (random) occurences from the dict \"\"\"\n",
    "    count = 1\n",
    "    sliced = {}\n",
    "    for key, value in mydict.items():\n",
    "        sliced[key] = value\n",
    "        count += 1\n",
    "        if count > num:\n",
    "            break\n",
    "\n",
    "    return sliced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attached': 2,\n",
       " 'both': 49,\n",
       " 'disk': 1,\n",
       " 'disturbance,': 1,\n",
       " 'hereafter;': 1,\n",
       " 'numerous': 3,\n",
       " 'position': 14,\n",
       " 'quite': 10,\n",
       " 'stopped': 2,\n",
       " 'upright,': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slicedict(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def sortdict(mydict):\n",
    "    \"\"\" Sort in descend based on values count \"\"\"\n",
    "    return sorted(mydict.items(), key=lambda x: x[1], reverse=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3068), ('to', 2100), ('and', 1907), ('of', 1796), ('in', 987)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortdict(data)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More than a single step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "mrjob can be configured to run different steps\n",
    "\n",
    "for each step you can specify which part has to be executed\n",
    "and the method to use within the class you wrote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def steps(self):\n",
    "    return [\n",
    "        MRStep(\n",
    "            mapper=self.mapper_get_words,\n",
    "            combiner=self.combiner_count_words,\n",
    "            reducer=self.reducer_count_words),\n",
    "        MRStep(reducer=self.reducer_find_max_word)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This could be an improvement for our mrjob extension:\n",
    "\n",
    "- if steps method is provided, override the default written by the extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A quick note\n",
    "\n",
    "* With MrJob you cannot connect to a **remote** Hadoop cluster. \n",
    "    - Hadoop does not allow job submissions (class or executables) from outside.\n",
    "* On the contrary EMR on Amazon can be accessible from your laptop.\n",
    "    - Amazon created the [boto api](http://boto.readthedocs.org/en/latest/ref/emr.html) to solve the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Runners\n",
    "\n",
    "http://mrjob.readthedocs.org/en/latest/guides/runners.html\n",
    "\n",
    "You cannot use the programmatic runner functionality in the same file as your job class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## RUNNER: JUST AN EXAMPLE\n",
    "\n",
    "# Load class for mapreduce\n",
    "from mypackage import SomeClassWeWrote\n",
    "#from job import MRcoverage\n",
    "from mrjob.util import log_to_stream\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Create object\n",
    "    mrjob = SomeClassWeWrote(args=[   \\\n",
    "        '-r', 'inline',\n",
    "        #'-r', 'local',\n",
    "        # '-r', 'hadoop',\n",
    "        # '--jobconf=mapreduce.job.maps=10',\n",
    "        # '--jobconf=mapreduce.job.reduces=4'\n",
    "        #'--jobconf=stream.recordreader.compression=bz2'\n",
    "        ])\n",
    "\n",
    "    # Run and handle output\n",
    "    with mrjob.make_runner() as runner:\n",
    "\n",
    "        # Redirect hadoop logs to stderr\n",
    "        log_to_stream(\"mrjob\")\n",
    "        # Execute the job\n",
    "        runner.run()\n",
    "\n",
    "        # Do something with stream output (e.g. file, database, etc.)\n",
    "        for line in runner.stream_output():\n",
    "            key, value = mrjob.parse_output_line(line)\n",
    "            print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap with Mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You should really read the [documentation of the latest version](http://mrjob.readthedocs.org/en/latest/).\n",
    "\n",
    "It covers every need without getting too much complicated.\n",
    "There are many other options and advanced behaviours to discover.\n",
    "\n",
    "<small>\n",
    "Note 1: We are using the most recent version (*release* **v.0.5.0dev** ) because its the first one to support Python 3.\n",
    "\n",
    "Note 2: Developer are there to help you, see my case in https://github.com/Yelp/mrjob/issues/1142\n",
    "</small>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* More documentation than any other framework or library\n",
    "* Write code in a single class (per Hadoop job)\n",
    "    * Map and Reduce are single methods\n",
    "    * Very clean and simple\n",
    "* Advanced configuration\n",
    "    * Configure multiple steps\n",
    "    * Handle command line options inside the python code (see docs)\n",
    "* Easily wrap input/output \n",
    "    * No data copy required with HDFS\n",
    "* Hadoop logs or errors directly into the script output\n",
    "* **Switch environment without changing the code...!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cons**\n",
    "\n",
    "* Doesn’t give you the same level of access to Hadoop APIs \n",
    "    - Better: Dumbo and Pydoop\n",
    "    - Other libraries can be faster if you use typedbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison\n",
    "<img src='http://blog.cloudera.com/wp-content/uploads/2013/01/features.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance\n",
    "<img src='http://blog.cloudera.com/wp-content/uploads/2013/01/performance.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "source: http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One final note*: \n",
    "> Open source is a great thing\n",
    "\n",
    "The community listens to you:\n",
    "https://github.com/Yelp/mrjob/issues/1142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of Chapter"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
