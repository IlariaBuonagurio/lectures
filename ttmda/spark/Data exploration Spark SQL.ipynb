{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Introduction to Spark SQL\n",
    "\n",
    "Spark SQL is the component of Spark and provides a SQL like interface.  \n",
    "In this tutorial we will show how use this component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Initialize SQL context:  \n",
    "1 - Import SQL context.  \n",
    "2 - Create Context.  \n",
    "3 - Set flag binaryAsString, this flag tells Spark SQL to treat binary-encoded data as strings.  \n",
    "4 - Set flag useDataSourceApi, for compatibility to parquet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "sqlCtx.sql(\"SET spark.sql.parquet.binaryAsString=true\")\n",
    "sqlCtx.sql(\"SET spark.sql.parquet.useDataSourceApi=false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load dataset file wiki_parquet into hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -put /notebooks/data/wiki_parquet wiki_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load dataset as RDD from hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikiData = sqlCtx.parquetFile(\"wiki_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count rows of dataset to verify RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39365L"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Register RDD as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikiData.registerTempTable(\"wikiData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count with Spark SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39365"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = sqlCtx.sql(\"SELECT COUNT(*) AS pageCount FROM wikiData\").collect()\n",
    "result[0].pageCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL can be a powerfull tool from performing complex aggregations. For example, the following query returns the top 10 usersnames by the number of pages they created.\n",
    "Command to avoid java.lang.OutOfMemoryError (restart pyspark with memory limits):\n",
    "usb/$ spark/bin/pyspark --driver-memory 1G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(username=u'Waacstats', cnt=2003),\n",
       " Row(username=u'Cydebot', cnt=949),\n",
       " Row(username=u'BattyBot', cnt=939),\n",
       " Row(username=u'Yobot', cnt=890),\n",
       " Row(username=u'Addbot', cnt=853),\n",
       " Row(username=u'Monkbot', cnt=668),\n",
       " Row(username=u'ChrisGualtieri', cnt=438),\n",
       " Row(username=u'RjwilmsiBot', cnt=387),\n",
       " Row(username=u'OccultZone', cnt=377),\n",
       " Row(username=u'ClueBot NG', cnt=353)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlCtx.sql(\"SELECT username, COUNT(*) AS cnt FROM wikiData WHERE username <> '' GROUP BY username ORDER BY cnt DESC LIMIT 10\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try write fellowing query.  \n",
    "How many articles contain the word “california”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(c0=1145)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Solution\n",
    "sqlCtx.sql(\"SELECT count(*) FROM wikiData where text like '%california%'\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
