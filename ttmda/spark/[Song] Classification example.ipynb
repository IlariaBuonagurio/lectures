{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Decision Tree and Naive Bayes example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MLlib libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "import pyspark.mllib.linalg\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and pre-processing\n",
    "The **Iris flower data set** or Fisher's Iris data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\". The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawData = sc.textFile(\"data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.1', '3.5', '1.4', '0.2', 'Iris-setosa']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "val splitlines = rawData.map(lines => {\n",
    "    lines.split(',')\n",
    "  })\n",
    "splitlines.first()\n",
    "'''\n",
    "splitlines = rawData.map(lambda lines: lines.split(','))\n",
    "splitlines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,[5.1,3.5,1.4,0.2])\n",
      "(1.0,[4.9,3.0,1.4,0.2])\n",
      "(1.0,[4.7,3.2,1.3,0.2])\n",
      "(1.0,[4.6,3.1,1.5,0.2])\n",
      "(1.0,[5.0,3.6,1.4,0.2])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val Data = splitlines.map { col =>   \n",
    "     val species = col(col.size - 1)                       \n",
    "     val label = if (species == \"Iris-versicolor\") 0.toInt else if (species == \"Iris-setosa\") 1.toInt else 2.toInt\n",
    "     val features = col.slice(0, col.size - 1).map(_.toDouble)\n",
    "     LabeledPoint(label, Vectors.dense(features))\n",
    "}\n",
    "Data.take(5).foreach(println)\n",
    "'''\n",
    "def iris_label(argument):\n",
    "    iris = {\n",
    "        \"Iris-versicolor\": 0,\n",
    "        \"Iris-setosa\": 1,\n",
    "    }\n",
    "    return iris.get(argument, 2)\n",
    "\n",
    "def estractor(col):\n",
    "    species = col[-1]\n",
    "    label = iris_label(species)\n",
    "    features = [float(x) for x in col[:-1]]\n",
    "    return LabeledPoint(label, Vectors.dense(features))\n",
    "    \n",
    "Data = splitlines.map(estractor)\n",
    "\n",
    "for x in Data.take(5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets (40% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "(1.0,[5.1,3.5,1.4,0.2])\n",
      "(1.0,[4.9,3.0,1.4,0.2])\n",
      "(1.0,[5.0,3.6,1.4,0.2])\n",
      "(1.0,[5.4,3.9,1.7,0.4])\n",
      "(1.0,[4.6,3.4,1.4,0.3])\n",
      "Test Data\n",
      "(1.0,[4.7,3.2,1.3,0.2])\n",
      "(1.0,[4.6,3.1,1.5,0.2])\n",
      "(1.0,[5.0,3.4,1.5,0.2])\n",
      "(1.0,[4.4,2.9,1.4,0.2])\n",
      "(1.0,[4.9,3.1,1.5,0.1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val splits = Data.randomSplit(Array(0.6, 0.4), seed = 11L)\n",
    "val trainingData = splits(0).cache()\n",
    "val testData = splits(1)\n",
    "println(\"Training Data\")\n",
    "trainingData.take(5).foreach(println)\n",
    "println(\"Test Data\")\n",
    "testData.take(5).foreach(println)\n",
    "'''\n",
    "splits = Data.randomSplit([0.6, 0.4], seed = 11)\n",
    "trainingData = splits[0].cache()\n",
    "testData = splits[1]\n",
    "print(\"Training Data\")\n",
    "for x in trainingData.take(5):\n",
    "    print(x)\n",
    "print(\"Test Data\")\n",
    "for x in testData.take(5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Decision Tree\n",
    "Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling and are able to capture nonlinearities and feature interactions. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks. MLlib supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 13 nodes\n",
      "  If (feature 2 <= 3.5)\n",
      "   If (feature 1 <= 2.6)\n",
      "    If (feature 0 <= 4.8)\n",
      "     Predict: 1.0\n",
      "    Else (feature 0 > 4.8)\n",
      "     Predict: 0.0\n",
      "   Else (feature 1 > 2.6)\n",
      "    Predict: 1.0\n",
      "  Else (feature 2 > 3.5)\n",
      "   If (feature 3 <= 1.7)\n",
      "    If (feature 2 <= 5.3)\n",
      "     Predict: 0.0\n",
      "    Else (feature 2 > 5.3)\n",
      "     Predict: 2.0\n",
      "   Else (feature 3 > 1.7)\n",
      "    If (feature 2 <= 5.0)\n",
      "     Predict: 2.0\n",
      "    Else (feature 2 > 5.0)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val numClasses = 3\n",
    "val categoricalFeaturesInfo = Map[Int, Int]()\n",
    "val impurity = \"entropy\"\n",
    "val maxDepth = 3\n",
    "val maxBins = 10\n",
    "val dtModel = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "  impurity, maxDepth, maxBins)\n",
    "println(dtModel.toDebugString)\n",
    "'''\n",
    "numClasses = 3\n",
    "categoricalFeaturesInfo = {} #Map[Int, Int]()\n",
    "impurity = \"entropy\"\n",
    "maxDepth = 3\n",
    "maxBins = 10\n",
    "dtModel = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)\n",
    "print(dtModel.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-fc931e5b6dee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprintln\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m '''\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mbcModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mdtTotalCorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainingData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mpoint\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdcModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mbroadcast\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[0mbe\u001b[0m \u001b[0msent\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \"\"\"\n\u001b[1;32m--> 698\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pickled_broadcast_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccum_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sc, value, pickle_registry, path)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jbroadcast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadBroadcastFromFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pickle_registry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_registry\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/broadcast.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(self, value, f)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         raise Exception(\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[1;34m\"It appears that you are attempting to reference SparkContext from a broadcast \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m             \u001b[1;34m\"variable, action, or transforamtion. SparkContext can only be used on the driver, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[1;34m\"not in code that it run on workers. For more information, see SPARK-5063.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforamtion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "'''\n",
    "val dtTotalCorrect = trainingData.map { point =>\n",
    "  if (dtModel.predict(point.features) == point.label) 1 else 0\n",
    "  }.sum\n",
    "\n",
    "println(dtTotalCorrect)\n",
    "println(trainingData.count)\n",
    "'''\n",
    "bcModel = sc.broadcast(dtModel)\n",
    "dtTotalCorrect = trainingData.map(lambda point: dcModel.predict(point.features))\n",
    "\n",
    "'''\n",
    "for x in trainingData.collect():\n",
    "    print(dtModel.predict(x.features))\n",
    "'''\n",
    "\n",
    "print(dtTotalCorrect)\n",
    "#print(trainingData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
