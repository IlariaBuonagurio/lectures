{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster analysis example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MLlib libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyspark.mllib.clustering\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "import pyspark.mllib.linalg\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and pre-processing\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\".\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawData = sc.textFile(\"data/iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code splits the CSV lines into columns and removes the final column. The remaining values\n",
    "are converted to an array of numeric values (Double objects), and emitted with the final label column in a\n",
    "tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rawData.map(_.split(',').last).countByValue().toSeq.sortBy(_._2).reverse.foreach(println)\n",
    "for x in sorted(rawData.map(lambda x:x.split(',')[-1]).countByValue().items(), key = lambda data:data[1], reverse=True):\n",
    "    print(x)\n",
    "'''\n",
    "val labelsAndData = rawData.map { line =>\n",
    "    val buffer = line.split(',').toBuffer\n",
    "    val label = buffer.remove(buffer.length - 1)\n",
    "    val vector = Vectors.dense(buffer.map(_.toDouble).toArray)\n",
    "    (label, vector)\n",
    "}\n",
    "\n",
    "val data = labelsAndData.values.cache()\n",
    "\n",
    "'''\n",
    "\n",
    "def estractLabelVector(line):\n",
    "    buff = line.split(',')\n",
    "    label = buff[-1]\n",
    "    vector = Vectors.dense([float(c) for c in buff[:-1]])\n",
    "    return (label, vector)\n",
    "\n",
    "labelsAndData = rawData.map(estractLabelVector)\n",
    "\n",
    "data = labelsAndData.values().cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "val numClusters = 2\n",
    "val numIterations = 1\n",
    "val model = KMeans.train(data,numClusters,numIterations)\n",
    "'''\n",
    "numClusters = 2\n",
    "numIterations = 1\n",
    "model = KMeans.train(data,numClusters,numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.clusterCenters.foreach(println)\n",
    "for x in model.clusterCenters:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "val clusterLabelCount = labelsAndData.map { case (label, datum) =>\n",
    "  val cluster = model.predict(datum)\n",
    "  (cluster, label)}.countByValue()\n",
    "  clusterLabelCount.toSeq.sorted.foreach { case ((cluster, label), count) =>\n",
    "  println(f\"$cluster%1s$label%18s$count%8s\")\n",
    "  }\n",
    "'''\n",
    "\n",
    "def match(data):\n",
    "    cluster = model.predict(data[1])\n",
    "    return (cluster, data[0])\n",
    "\n",
    "clusterLabelCount = labelsAndData.map(match).countByValue()\n",
    "\n",
    "for key in clusterLabelCount.keys():\n",
    "    print(\"%1s%18s%8s\" % (key[0], key[1], clusterLabelCount[key]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write sample and total result in a directory hadoop style and coalesce all in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''  \n",
    "val sample = data.map(datum => model.predict(datum) + \",\" +datum.toArray.mkString(\",\")).sample(false,0.05)\n",
    "val total = data.map(datum => model.predict(datum) + \",\" +datum.toArray.mkString(\",\"))\n",
    "sample.saveAsTextFile(\"share/sample\")\n",
    "sample.coalesce(1).saveAsTextFile(\"share/sample_coalesce\")\n",
    "'''\n",
    "total = data.map(lambda datum: str(model.predict(datum)) + \",\" +\",\".join([str(c) for c in datum]))\n",
    "sample = total.sample(False,0.05)\n",
    "sample.saveAsTextFile(\"outPython/sample\")\n",
    "sample.coalesce(1).saveAsTextFile(\"outPython/sample_coalesce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of K\n",
    "A clustering could be considered good if each data point were near to its closest centroid. So, we define a\n",
    "Euclidean distance function, and a function that returns the distance from a data point to its nearest cluster’s\n",
    "centroid. From this, it’s possible to define a function that measures the average distance to centroid, for a\n",
    "model built with a given k. This is an internal quality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def distance(a: Vector, b: Vector) =math.sqrt(a.toArray.zip(b.toArray).map(p => p._1 - p._2).map(d => d * d).sum)\n",
    "import numpy as np\n",
    "\n",
    "def distance(a, b):\n",
    "    return np.linalg.norm(np.asarray(a)-np.asarray(b))\n",
    "\n",
    "'''\n",
    "def distToCentroid(datum: Vector, model: KMeansModel) = {\n",
    "      val cluster = model.predict(datum)\n",
    "      val centroid = model.clusterCenters(cluster)\n",
    "      distance(centroid, datum)\n",
    "}   \n",
    "'''\n",
    "def distToCentroid(datum, model):\n",
    "    cluster = model.predict(datum)\n",
    "    centroid = model.clusterCenters[cluster]\n",
    "    return distance(centroid, datum)\n",
    "\n",
    "'''\n",
    "def clusteringScore(data: RDD[Vector], k: Int): Double = {\n",
    "      val kmeans = new KMeans()\n",
    "      kmeans.setK(k)\n",
    "      kmeans.setRuns(30)\n",
    "      kmeans.setEpsilon(1.0e-6)\n",
    "      val model = kmeans.run(data)\n",
    "      data.map(datum => distToCentroid(datum, model)).mean()\n",
    "}\n",
    "'''\n",
    "def clusteringScore(data, k):\n",
    "    model = KMeans.train(data, k, runs=30, epsilon=1.0e-6)\n",
    "    return data.map(lambda datum: distToCentroid(datum, model)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate score values for different k from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#(1 to 10 by 1).map(k => (k, clusteringScore(data, k))).foreach(println)\n",
    "y = []\n",
    "for k in range(10):\n",
    "    score = clusteringScore(data,k+1)\n",
    "    y.append(score)\n",
    "    print((k+1,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.plot(range(1,11),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "Since Euclidean distance is used, the clusters will be influenced strongly by the magnitudes of the variables, especially by outliers. Normalizing removes this bias. We can normalize each feature by converting it to a standard score. This means subtracting the mean of the feature’s values from each value, and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def buildNormalizationFunction(data: RDD[Vector]): (Vector => Vector) = {\n",
    "    val dataAsArray = data.map(_.toArray)\n",
    "    val numCols = dataAsArray.first().length\n",
    "    val n = dataAsArray.count()\n",
    "    val sums = dataAsArray.reduce(\n",
    "      (a, b) => a.zip(b).map(t => t._1 + t._2))\n",
    "    val sumSquares = dataAsArray.fold(\n",
    "        new Array[Double](numCols)\n",
    "      )(\n",
    "        (a, b) => a.zip(b).map(t => t._1 + t._2 * t._2)\n",
    "      )\n",
    "    val stdevs = sumSquares.zip(sums).map {\n",
    "      case (sumSq, sum) => math.sqrt(n * sumSq - sum * sum) / n\n",
    "    }\n",
    "    val means = sums.map(_ / n)\n",
    "\n",
    "    (datum: Vector) => {\n",
    "      val normalizedArray = (datum.toArray, means, stdevs).zipped.map(\n",
    "        (value, mean, stdev) =>\n",
    "          if (stdev <= 0)  (value - mean) else  (value - mean) / stdev\n",
    "      )\n",
    "      Vectors.dense(normalizedArray)\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "def buildNormalizationFunction(data):\n",
    "    dataAsArray = data.map(lambda datum: np.asarray(datum))\n",
    "    numCols = dataAsArray.first().size\n",
    "    n = dataAsArray.count()\n",
    "    sums = dataAsArray.reduce(lambda a,b: [t1 + t2 for t1, t2 in zip(a,b)])\n",
    "    sumSquares = dataAsArray.reduce(lambda a,b: [t1 + t2*t2 for t1, t2 in zip(a,b)])\n",
    "    stdevs = [(n * sumSq - sum**2)**(1/2)/n for sumSq, sum in zip(sumSquares, sums)]\n",
    "    means = [s/n for s in sums]\n",
    "    return dataAsArray.map(lambda datum :\n",
    "                           Vectors.dense([value - mean if stdev == 0 else (value - mean) / stdev\n",
    "                                          for value, mean, stdev in zip(datum, means, stdevs)])\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizedData = buildNormalizationFunction(data).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(1 to 10 by 1).map(k =>\n",
    "      (k, clusteringScore(normalizedData, k))).foreach(println)\n",
    "'''\n",
    "y = []\n",
    "for k in range(10):\n",
    "    score = clusteringScore(normalizedData,k+1)\n",
    "    y.append(score)\n",
    "    print((k+1,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.plot(range(1,11),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Entropy measure\n",
    "The are different metrics for homogeneity. Entropy is used here for illustration. A good clustering would have clusters whose collections of labels are homogeneous and so have low entropy. A weighted average of entropy can therefore be used as a cluster score. This is an external quality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def entropy(counts: Iterable[Int]) = {\n",
    "    val values = counts.filter(_ > 0)\n",
    "    val n: Double = values.sum\n",
    "    values.map { v =>\n",
    "    val p = v / n \n",
    "    -p * math.log(p)\n",
    "    }.sum\n",
    "}\n",
    "'''\n",
    "import math\n",
    "def entropy(counts):\n",
    "    values = [float(x) for x in filter(lambda x: x>0, counts)]\n",
    "    n = float(sum(values))\n",
    "    return sum(map(lambda v: - v/n * math.log(v/n), values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def buildCategoricalAndLabelFunction(rawData: RDD[String]): (String => (String,Vector)) = {\n",
    "val splitData = rawData.map(_.split(','))\n",
    "   (line: String) => {\n",
    "      val buffer = line.split(',').toBuffer \n",
    "      val label = buffer.remove(buffer.length - 1)\n",
    "      val vector = buffer.map(_.toDouble)\n",
    "      (label, Vectors.dense(vector.toArray))\n",
    "    }\n",
    "}\n",
    "'''\n",
    "def buildCategoricalAndLabelFunction(rawData):\n",
    "    return rawData.map(estractLabelVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for choosing K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def clusteringScore3(normalizedLabelsAndData: RDD[(String,Vector)], k: Int) = {\n",
    "    val kmeans = new KMeans()\n",
    "    kmeans.setK(k)\n",
    "    kmeans.setRuns(10)\n",
    "    kmeans.setEpsilon(1.0e-6)\n",
    "    val model = kmeans.run(normalizedLabelsAndData.values)\n",
    "    val labelsAndClusters = normalizedLabelsAndData.mapValues(model.predict)\n",
    "    val clustersAndLabels = labelsAndClusters.map(_.swap)\n",
    "    val labelsInCluster = clustersAndLabels.groupByKey().values\n",
    "    val labelCounts = labelsInCluster.map(_.groupBy(l => l).map(_._2.size))\n",
    "    val n = normalizedLabelsAndData.count()\n",
    "    labelCounts.map(m => m.sum * entropy(m)).sum / n\n",
    "}\n",
    "\n",
    "val parseFunction = buildCategoricalAndLabelFunction(rawData)\n",
    "val labelsAndData = rawData.map(parseFunction)\n",
    "val normalizedLabelsAndData =\n",
    "      labelsAndData.mapValues(buildNormalizationFunction(labelsAndData.values)).cache()\n",
    "(1 to 10 by 1).map(k =>\n",
    "      (k, clusteringScore3(normalizedLabelsAndData, k))).foreach(println)\n",
    "normalizedLabelsAndData.unpersist()\n",
    "'''\n",
    "from itertools import groupby\n",
    "def clusteringScore3(normalizedLabelsAndData, k):\n",
    "    model = KMeans.train(normalizedLabelsAndData.values(), k, runs=30, epsilon=1.0e-6)\n",
    "    labelsAndClusters = normalizedLabelsAndData.mapValues(model.predict)\n",
    "    clustersAndLabels = labelsAndClusters.map(lambda x: (x[1],x[0]))\n",
    "    labelsInCluster = clustersAndLabels.groupByKey().values()\n",
    "    labelCounts = labelsInCluster.map(lambda x: [len(list(group)) for key, group in groupby(x, lambda l:l)])\n",
    "    n = float(normalizedLabelsAndData.count())\n",
    "    return labelCounts.map(lambda m: sum(m) * entropy(m)).sum() / n\n",
    "    \n",
    "def buildNormalizationFunctionWithLabels(data):\n",
    "    dataAsArray = data.values().map(lambda datum: np.asarray(datum))\n",
    "    numCols = dataAsArray.first().size\n",
    "    n = dataAsArray.count()\n",
    "    sums = dataAsArray.reduce(lambda a,b: [t1 + t2 for t1, t2 in zip(a,b)])\n",
    "    sumSquares = dataAsArray.reduce(lambda a,b: [t1 + t2*t2 for t1, t2 in zip(a,b)])\n",
    "    stdevs = [(n * sumSq - sum**2)**(1/2)/n for sumSq, sum in zip(sumSquares, sums)]\n",
    "    means = [s/n for s in sums]\n",
    "    return data.map(lambda datum :\n",
    "                           (datum[0], Vectors.dense([value - mean if stdev == 0 else (value - mean) / stdev\n",
    "                                                     for value, mean, stdev in zip(datum[1], means, stdevs)]))\n",
    "                          )\n",
    "    \n",
    "labelsAndData = buildCategoricalAndLabelFunction(rawData)\n",
    "normalizedLabelsAndData = buildNormalizationFunctionWithLabels(labelsAndData)\n",
    "y = []\n",
    "for k in range(10):\n",
    "    score = clusteringScore3(normalizedLabelsAndData,k+1)\n",
    "    y.append(score)\n",
    "    print((k+1,score))\n",
    "normalizedLabelsAndData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.plot(range(1,11),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
