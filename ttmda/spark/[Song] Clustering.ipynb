{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster analysis example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MLlib libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.mllib.clustering\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "import pyspark.mllib.linalg\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData = sc.textFile(\"data/iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code splits the CSV lines into columns and removes the final column. The remaining values\n",
    "are converted to an array of numeric values (Double objects), and emitted with the final label column in a\n",
    "tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Iris-setosa', 50)\n",
      "('Iris-virginica', 50)\n",
      "('Iris-versicolor', 50)\n"
     ]
    }
   ],
   "source": [
    "#rawData.map(_.split(',').last).countByValue().toSeq.sortBy(_._2).reverse.foreach(println)\n",
    "for x in sorted(rawData.map(lambda x:x.split(',')[-1]).countByValue().items(), key = lambda data:data[1], reverse=True):\n",
    "    print(x)\n",
    "'''\n",
    "val labelsAndData = rawData.map { line =>\n",
    "    val buffer = line.split(',').toBuffer\n",
    "    val label = buffer.remove(buffer.length - 1)\n",
    "    val vector = Vectors.dense(buffer.map(_.toDouble).toArray)\n",
    "    (label, vector)\n",
    "}\n",
    "\n",
    "val data = labelsAndData.values.cache()\n",
    "\n",
    "'''\n",
    "\n",
    "def estractLabelVector(line):\n",
    "    buff = line.split(',')\n",
    "    label = buff[-1]\n",
    "    vector = Vectors.dense([float(c) for c in buff[:-1]])\n",
    "    return (label, vector)\n",
    "\n",
    "labelsAndData = rawData.map(estractLabelVector)\n",
    "\n",
    "data = labelsAndData.values().cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "val numClusters = 2\n",
    "val numIterations = 1\n",
    "val model = KMeans.train(data,numClusters,numIterations)\n",
    "'''\n",
    "numClusters = 2\n",
    "numIterations = 1\n",
    "model = KMeans.train(data,numClusters,numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.00566038  3.36981132  1.56037736  0.29056604]\n",
      "[ 6.30103093  2.88659794  4.95876289  1.69587629]\n"
     ]
    }
   ],
   "source": [
    "#model.clusterCenters.foreach(println)\n",
    "for x in model.clusterCenters:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply cluster model\n",
    "The following code uses the model to assign each observation to a cluster, counts occurrences of cluster and\n",
    "label pairs, and prints them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Iris-setosa      50\n",
      "1    Iris-virginica      50\n",
      "1   Iris-versicolor      47\n",
      "0   Iris-versicolor       3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "val clusterLabelCount = labelsAndData.map { case (label, datum) =>\n",
    "  val cluster = model.predict(datum)\n",
    "  (cluster, label)}.countByValue()\n",
    "  clusterLabelCount.toSeq.sorted.foreach { case ((cluster, label), count) =>\n",
    "  println(f\"$cluster%1s$label%18s$count%8s\")\n",
    "  }\n",
    "'''\n",
    "\n",
    "def match(data):\n",
    "    cluster = model.predict(data[1])\n",
    "    return (cluster, data[0])\n",
    "\n",
    "clusterLabelCount = labelsAndData.map(match).countByValue()\n",
    "\n",
    "for key in clusterLabelCount.keys():\n",
    "    print(\"%1s%18s%8s\" % (key[0], key[1], clusterLabelCount[key]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write sample and total result in a directory hadoop style and coalesce all in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o58.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/jovyan/work/share/sample already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1404)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1383)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1383)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1383)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:519)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:47)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5708dd14d8ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mdatum\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\",\"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"share/sample\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"share/sample_coalesce\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1484\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o58.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/jovyan/work/share/sample already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1404)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1383)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1383)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1383)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:519)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:47)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "'''  \n",
    "val sample = data.map(datum => model.predict(datum) + \",\" +datum.toArray.mkString(\",\")).sample(false,0.05)\n",
    "val total = data.map(datum => model.predict(datum) + \",\" +datum.toArray.mkString(\",\"))\n",
    "sample.saveAsTextFile(\"share/sample\")\n",
    "sample.coalesce(1).saveAsTextFile(\"share/sample_coalesce\")\n",
    "'''\n",
    "total = data.map(lambda datum: str(model.predict(datum)) + \",\" +\",\".join([str(c) for c in datum]))\n",
    "sample = total.sample(False,0.05)\n",
    "sample.saveAsTextFile(\"share/sample\")\n",
    "sample.coalesce(1).saveAsTextFile(\"share/sample_coalesce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Choice of K\n",
    "A clustering could be considered good if each data point were near to its closest centroid. So, we define a\n",
    "Euclidean distance function, and a function that returns the distance from a data point to its nearest cluster’s\n",
    "centroid. From this, it’s possible to define a function that measures the average distance to centroid, for a\n",
    "model built with a given k. This is an internal quality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def distance(a: Vector, b: Vector) =math.sqrt(a.toArray.zip(b.toArray).map(p => p._1 - p._2).map(d => d * d).sum)\n",
    "import numpy as np\n",
    "\n",
    "def distance(a, b):\n",
    "    return np.linalg.norm(np.asarray(a)-np.asarray(b))\n",
    "\n",
    "'''\n",
    "def distToCentroid(datum: Vector, model: KMeansModel) = {\n",
    "      val cluster = model.predict(datum)\n",
    "      val centroid = model.clusterCenters(cluster)\n",
    "      distance(centroid, datum)\n",
    "}   \n",
    "'''\n",
    "def distToCentroid(datum, model):\n",
    "    cluster = model.predict(datum)\n",
    "    centroid = model.clusterCenters[cluster]\n",
    "    return distance(centroid, datum)\n",
    "\n",
    "'''\n",
    "def clusteringScore(data: RDD[Vector], k: Int): Double = {\n",
    "      val kmeans = new KMeans()\n",
    "      kmeans.setK(k)\n",
    "      kmeans.setRuns(30)\n",
    "      kmeans.setEpsilon(1.0e-6)\n",
    "      val model = kmeans.run(data)\n",
    "      data.map(datum => distToCentroid(datum, model)).mean()\n",
    "}\n",
    "'''\n",
    "def clusteringScore(data, k):\n",
    "    model = KMeans.train(data, k, runs=30, epsilon=1.0e-6)\n",
    "    return data.map(lambda datum: distToCentroid(datum, model)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate score values for different k from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1.9440683605553895)\n",
      "(2, 0.85557776952665299)\n",
      "(3, 0.64803049049344341)\n",
      "(4, 0.55738477273333131)\n",
      "(5, 0.50965219511182869)\n",
      "(6, 0.4670733963396439)\n",
      "(7, 0.43616625646364982)\n",
      "(8, 0.41086768314893807)\n",
      "(9, 0.39617960125111995)\n",
      "(10, 0.38159011216911809)\n"
     ]
    }
   ],
   "source": [
    "#(1 to 10 by 1).map(k => (k, clusteringScore(data, k))).foreach(println)\n",
    "for k in range(10):\n",
    "    print((k+1,clusteringScore(data,k+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Data Normalization\n",
    "Since Euclidean distance is used, the clusters will be influenced strongly by the magnitudes of the variables, especially by outliers. Normalizing removes this bias. We can normalize each feature by converting it to a standard score. This means subtracting the mean of the feature’s values from each value, and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def buildNormalizationFunction(data: RDD[Vector]): (Vector => Vector) = {\n",
    "    val dataAsArray = data.map(_.toArray)\n",
    "    val numCols = dataAsArray.first().length\n",
    "    val n = dataAsArray.count()\n",
    "    val sums = dataAsArray.reduce(\n",
    "      (a, b) => a.zip(b).map(t => t._1 + t._2))\n",
    "    val sumSquares = dataAsArray.fold(\n",
    "        new Array[Double](numCols)\n",
    "      )(\n",
    "        (a, b) => a.zip(b).map(t => t._1 + t._2 * t._2)\n",
    "      )\n",
    "    val stdevs = sumSquares.zip(sums).map {\n",
    "      case (sumSq, sum) => math.sqrt(n * sumSq - sum * sum) / n\n",
    "    }\n",
    "    val means = sums.map(_ / n)\n",
    "\n",
    "    (datum: Vector) => {\n",
    "      val normalizedArray = (datum.toArray, means, stdevs).zipped.map(\n",
    "        (value, mean, stdev) =>\n",
    "          if (stdev <= 0)  (value - mean) else  (value - mean) / stdev\n",
    "      )\n",
    "      Vectors.dense(normalizedArray)\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "def buildNormalizationFunction(data):\n",
    "    dataAsArray = data.map(lambda datum: np.asarray(datum))\n",
    "    numCols = dataAsArray.first().size\n",
    "    n = dataAsArray.count()\n",
    "    sums = dataAsArray.reduce(lambda a,b: [t1 + t2 for t1, t2 in zip(a,b)])\n",
    "    sumSquares = dataAsArray.reduce(lambda a,b: [t1 + t2*t2 for t1, t2 in zip(a,b)])\n",
    "    stdevs = [(n * sumSq - sum**2)**(1/2)/n for sumSq, sum in zip(sumSquares, sums)]\n",
    "    means = [s/n for s in sums]\n",
    "    return dataAsArray.map(lambda datum :\n",
    "                           Vectors.dense([value - mean if stdev == 0 else (value - mean) / stdev\n",
    "                                          for value, mean, stdev in zip(datum, means, stdevs)])\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalizedData = buildNormalizationFunction(data).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.036432654871608426)\n",
      "(2, 0.01634733527199109)\n",
      "(3, 0.010421227668008742)\n",
      "(4, 0.0087876646616176085)\n",
      "(5, 0.0078241687917766645)\n",
      "(6, 0.0071239033848412171)\n",
      "(7, 0.0066003290868659964)\n",
      "(8, 0.0061642170075782831)\n",
      "(9, 0.0057914506083838859)\n",
      "(10, 0.0056075297544857438)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "(1 to 10 by 1).map(k =>\n",
    "      (k, clusteringScore(normalizedData, k))).foreach(println)\n",
    "'''\n",
    "for k in range(10):\n",
    "    print((k+1,clusteringScore(normalizedData,k+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Definition of Entropy measure\n",
    "The are different metrics for homogeneity. Entropy is used here for illustration. A good clustering would have clusters whose collections of labels are homogeneous and so have low entropy. A weighted average of entropy can therefore be used as a cluster score. This is an external quality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def entropy(counts: Iterable[Int]) = {\n",
    "    val values = counts.filter(_ > 0)\n",
    "    val n: Double = values.sum\n",
    "    values.map { v =>\n",
    "    val p = v / n \n",
    "    -p * math.log(p)\n",
    "    }.sum\n",
    "}\n",
    "'''\n",
    "import math\n",
    "def entropy(counts):\n",
    "    values = filter(lambda x: x>0, counts)\n",
    "    n = float(sum(values))\n",
    "    return sum(map(lambda v: - v/n * math.log(v/n), values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def buildCategoricalAndLabelFunction(rawData: RDD[String]): (String => (String,Vector)) = {\n",
    "val splitData = rawData.map(_.split(','))\n",
    "   (line: String) => {\n",
    "      val buffer = line.split(',').toBuffer \n",
    "      val label = buffer.remove(buffer.length - 1)\n",
    "      val vector = buffer.map(_.toDouble)\n",
    "      (label, Vectors.dense(vector.toArray))\n",
    "    }\n",
    "}\n",
    "'''\n",
    "def buildCategoricalAndLabelFunction(rawData):\n",
    "    return rawData.map(estractLabelVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for choosing K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.0)\n",
      "(2, 0.0)\n",
      "(3, 0.0)\n",
      "(4, 0.0)\n",
      "(5, 0.0)\n",
      "(6, 0.0)\n",
      "(7, 0.0)\n",
      "(8, 0.0)\n",
      "(9, 0.0)\n",
      "(10, 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[8256] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def clusteringScore3(normalizedLabelsAndData: RDD[(String,Vector)], k: Int) = {\n",
    "    val kmeans = new KMeans()\n",
    "    kmeans.setK(k)\n",
    "    kmeans.setRuns(10)\n",
    "    kmeans.setEpsilon(1.0e-6)\n",
    "    val model = kmeans.run(normalizedLabelsAndData.values)\n",
    "    val labelsAndClusters = normalizedLabelsAndData.mapValues(model.predict)\n",
    "    val clustersAndLabels = labelsAndClusters.map(_.swap)\n",
    "    val labelsInCluster = clustersAndLabels.groupByKey().values\n",
    "    val labelCounts = labelsInCluster.map(_.groupBy(l => l).map(_._2.size))\n",
    "    val n = normalizedLabelsAndData.count()\n",
    "    labelCounts.map(m => m.sum * entropy(m)).sum / n\n",
    "}\n",
    "\n",
    "val parseFunction = buildCategoricalAndLabelFunction(rawData)\n",
    "val labelsAndData = rawData.map(parseFunction)\n",
    "val normalizedLabelsAndData =\n",
    "      labelsAndData.mapValues(buildNormalizationFunction(labelsAndData.values)).cache()\n",
    "(1 to 10 by 1).map(k =>\n",
    "      (k, clusteringScore3(normalizedLabelsAndData, k))).foreach(println)\n",
    "normalizedLabelsAndData.unpersist()\n",
    "'''\n",
    "from itertools import groupby\n",
    "def clusteringScore3(normalizedLabelsAndData, k):\n",
    "    model = KMeans.train(normalizedLabelsAndData.values(), k, runs=30, epsilon=1.0e-6)\n",
    "    labelsAndClusters = normalizedLabelsAndData.mapValues(model.predict)\n",
    "    clustersAndLabels = labelsAndClusters.map(lambda x: (x[1],x[0]))\n",
    "    labelsInCluster = clustersAndLabels.groupByKey().values()\n",
    "    labelCounts = labelsInCluster.map(lambda x: [len(list(group)) for key, group in groupby(x, lambda l:l)])\n",
    "    n = float(normalizedLabelsAndData.count())\n",
    "    return labelCounts.map(lambda m: sum(m) * entropy(m)).sum() / n\n",
    "    \n",
    "def buildNormalizationFunctionWithLabels(data):\n",
    "    dataAsArray = data.values().map(lambda datum: np.asarray(datum))\n",
    "    numCols = dataAsArray.first().size\n",
    "    n = dataAsArray.count()\n",
    "    sums = dataAsArray.reduce(lambda a,b: [t1 + t2 for t1, t2 in zip(a,b)])\n",
    "    sumSquares = dataAsArray.reduce(lambda a,b: [t1 + t2*t2 for t1, t2 in zip(a,b)])\n",
    "    stdevs = [(n * sumSq - sum**2)**(1/2)/n for sumSq, sum in zip(sumSquares, sums)]\n",
    "    means = [s/n for s in sums]\n",
    "    return data.map(lambda datum :\n",
    "                           (datum[0], Vectors.dense([value - mean if stdev == 0 else (value - mean) / stdev\n",
    "                                                     for value, mean, stdev in zip(datum[1], means, stdevs)]))\n",
    "                          )\n",
    "    \n",
    "labelsAndData = buildCategoricalAndLabelFunction(rawData)\n",
    "normalizedLabelsAndData = buildNormalizationFunctionWithLabels(labelsAndData)\n",
    "for k in range(10):\n",
    "    print((k+1,clusteringScore3(normalizedLabelsAndData,k+1)))\n",
    "normalizedLabelsAndData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
