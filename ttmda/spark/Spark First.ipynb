{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Py)Spark Exercizes\n",
    "@stravanni\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark vs Pyspark?\n",
    "\n",
    "Spark is written in Scala. The 'native' API is in Scala.\n",
    "\n",
    "Pyspark is a very lightweight wrapper around the native API. (You can see its implementation [here](https://github.com/apache/spark/tree/master/python/pyspark).)\n",
    "\n",
    "---\n",
    "\n",
    "![](http://i.imgur.com/YlI8AqEl.png)\n",
    "\n",
    "[source](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to the (py)Spark\n",
    "\n",
    "1. The *spark context*: `sc`\n",
    "2. RDDs\n",
    "3. API: map reduce, and many more\n",
    "\n",
    "(non fare niente qua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##A. Wordcount\n",
    "1. read the file \"example.txt\", containing \"THE DIVINE COMEDY\"\n",
    "2. Select the 10 most frequent words, exluding che stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import nltk.corpus as corpus\n",
    "    stopwords = set(corpus.stopwords.words())\n",
    "except ImportError: \n",
    "    stopwords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'one', 413),\n",
       " (u'thou', 284),\n",
       " (u'may', 238),\n",
       " (u'great', 199),\n",
       " (u'made', 183),\n",
       " (u'would', 171),\n",
       " (u'upon', 166),\n",
       " (u'like', 166),\n",
       " (u'time', 159),\n",
       " (u'thy', 148)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrivere la soluzione qua\n",
    "# Most common words in \"THE DIVINE COMEDY\"\n",
    "stopwords = stopwords.union([\"dante\",\"etc._:\",\"dante's\",\"_the\",\"(_inf._\"])\n",
    "rdd = sc.textFile(\"file:///notebooks/DivineComedy.txt\")\n",
    "res = rdd .flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda word: word.strip().lower()) \\\n",
    "        .filter(lambda word: word not in stopwords) \\\n",
    "        .map(lambda word: (word, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .sortBy(lambda (word, count):count, ascending=False) \\\n",
    "        .take(10)\n",
    "#    .map(lambda (key, cnt): (cnt, key)) \\\n",
    "#    .top(10)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##B. Estimating Pi\n",
    "This code estimates π by \"throwing darts\" at a circle.\n",
    "\n",
    "1. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle.\n",
    "2. The fraction should be π / 4, so we use this to get our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 2.880000\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from random import random\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "def sample(p):\n",
    "    x, y = random(), random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "#print random()\n",
    "#sample(10)\n",
    "#a = [1,2,3]\n",
    "#map(lambda a: 1 if random()**2 + random()**2 < 1 else 0,a)\n",
    "#lambda a: 1 if random()**2 + random()**2 < 1 else 0\n",
    "\n",
    "sample_nums = sc.parallelize(xrange(0, NUM_SAMPLES))\n",
    "#sample = sample_nums.map(lambda a: 1 if 1<2 else 0)\n",
    "sample = sample_nums.map(sample)\n",
    "count = sample.reduce(lambda a, b: a + b)\n",
    "\n",
    "print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##C. TMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to extract the data from the line\n",
    "#based on position and filter out the invalid records\n",
    "def extractData(line):\n",
    "    val = line.strip()\n",
    "    (year, temp, q) = (str(val[15:19]), str(val[87:92]), str(val[92:93]))\n",
    "    if (temp != \"+9999\" and re.match(\"[01459]\", q)):\n",
    "        return [(year, temp)]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create an RDD from the input data in HDFS\n",
    "weatherData = sc.textFile(\"file:///notebooks/rta-pyspark-presentation-master/1902.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform the data to extract/filter and then find the max temperature\n",
    "temperature_per_year = weatherData.flatMap(extractData)\n",
    "max_temperature_per_year = temperature_per_year.reduceByKey(lambda a,b : a if int(a) > int(b) else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1902']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temperature_per_year.sortByKey().takeSample(False,10)\n",
    "years = temperature_per_year.map(lambda (y,t): y)\n",
    "years.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save the RDD back into HDFS\n",
    "max_temperature_per_year.saveAsTextFile(\"hdfs:///output\")\n",
    "#max_temperature_per_year.saveAsTextFile(\"hdfs:///output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Currently, pyspark doesn't support overwrite or append.\n",
    "\n",
    "- The function `saveAsTextFile` is\n",
    "a wrapper around `saveAsHadoopFile` and it's not possible overwrite existing files.\n",
    "\n",
    "#### in scala\n",
    "It is however trivial to do this using HDFS directly from Scala:\n",
    "```\n",
    "val hadoopConf = new org.apache.hadoop.conf.Configuration()\n",
    "\n",
    "val hdfs = org.apache.hadoop.fs.FileSystem.get(new java.net.URI(\"hdfs://localhost:9000\"), hadoopConf)\n",
    "```\n",
    "#### in shell\n",
    "- If you need to merge hdfs file, remember to use:\n",
    "[hadoop getMerge](https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/FileSystemShell.html#getmerge)\n",
    "- If you simply want to delete it:\n",
    "```\n",
    "hdfs dfs -rm -R \"hdfs:///output\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weatherData_ = sc.textFile(\"hdfs:///output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"('1902', '+0244')\"]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherData_.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 root supergroup          0 2015-04-01 10:05 /output/_SUCCESS\n",
      "-rw-r--r--   1 root supergroup         18 2015-04-01 10:05 /output/part-00000\n",
      "-rw-r--r--   1 root supergroup          0 2015-04-01 10:05 /output/part-00001\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15/04/01 10:05:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -rm -R \"hdfs:///output\"\n",
    "\n",
    "# http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html#rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
