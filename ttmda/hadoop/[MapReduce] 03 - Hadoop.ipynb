{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop\n",
    "(and Hadoop streaming)\n",
    "\n",
    "<center>\n",
    "<img src='http://www.datameer.com/images/technology/hadoop-pic1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**MapReduce** is a completely different paradigm \n",
    "\n",
    "* Solving a certain subset of parallelizable problems \n",
    "    - around the bottleneck of ingesting input data from disk\n",
    "* Traditional parallelism brings the data to the computing machine\n",
    "    - Map/reduce does the opposite, it brings the compute to the data\n",
    "* Input data is not stored on a separate storage system\n",
    "* Data exists in little pieces \n",
    "    - and is permanently stored on each computing node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MapReduce is the programming paradigm that allows massive scalability across thousands of servers.\n",
    "\n",
    "Its open source server implementation is the *Hadoop* cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Also always keep in mind that ***HDFS*** is fundamental to Hadoop \n",
    "\n",
    "* it provides the data chunking distribution across compute elements \n",
    "* necessary for map/reduce applications to be efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word count\n",
    "The '`Hello World`' for MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Among the simplest of full Hadoop jobs you can run\n",
    "\n",
    "<img src='http://www.glennklockwood.com/data-intensive/hadoop/wordcount-schematic.png'\n",
    "width='700'>\n",
    "<small>Reading ***Moby Dick*** </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works\n",
    "* The **MAP step** will take the raw text and convert it to key/value pairs\n",
    "    - Each key is a word\n",
    "    - All keys (words) will have a value of 1\n",
    "\n",
    "\n",
    "* The **REDUCE step** will combine all duplicate keys \n",
    "    - By adding up their values (sum)\n",
    "    - Every key (word) has a value of 1 (Map)\n",
    "    - Output is reduced to a list of unique keys\n",
    "    - Each key’s value corresponding to key's (word's) count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='http://disco.readthedocs.org/en/latest/_images/map_shuffle_reduce.png'>\n",
    "\n",
    "### Map function\n",
    "processes data and generates a set of  intermediate key/value pairs.\n",
    "### Reduce function \n",
    "merges all intermediate values  associated with the same intermediate key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A WordCount example \n",
    "*(with Java)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider doing a word count of the following file using  MapReduce:\n",
    "```\n",
    "Hello World Bye World\n",
    "Hello Hadoop Goodbye Hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The map function reads in words one at a time outputs (“word”, 1) for each parsed input word\n",
    "\n",
    "```\n",
    "(Hello, 1)\n",
    "(World, 1)\n",
    "(Bye, 1)\n",
    "(World, 1)\n",
    "(Hello, 1)\n",
    "(Hadoop, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The shuffle phase between map and reduce creates a  list of values associated with each key\n",
    "```\n",
    "(Bye, (1))\n",
    "(Goodbye, (1))\n",
    "(Hadoop, (1, 1))\n",
    "(Hello, (1, 1))\n",
    "(World, (1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce function sums the numbers in the list for each  key and outputs (word, count) pairs\n",
    "```\n",
    "(Bye, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 2)\n",
    "(Hello, 2)\n",
    "(World, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can you do this with Java?\n",
    "(the Hadoop framework native language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "// Imports\n",
    "package org.myorg;\n",
    "import java.io.IOException;\n",
    "import java.util.*;\n",
    "import org.apache.hadoop.*\n",
    "\n",
    "// Create JAVA class\n",
    "public class WordCount {\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Mapper function\n",
    "  public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      String line = value.toString();\n",
    "      StringTokenizer tokenizer = new StringTokenizer(line);\n",
    "      while (tokenizer.hasMoreTokens()) {\n",
    "        word.set(tokenizer.nextToken());\n",
    "        output.collect(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Reducer function\n",
    "  public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      int sum = 0;\n",
    "      while (values.hasNext()) {\n",
    "        sum += values.next().get();\n",
    "      }\n",
    "      output.collect(key, new IntWritable(sum));\n",
    "    }\n",
    "  }\n",
    "    \n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Main function\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    JobConf conf = new JobConf(WordCount.class);\n",
    "    conf.setJobName(\"wordcount\");\n",
    "\n",
    "    conf.setOutputKeyClass(Text.class);\n",
    "    conf.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    conf.setMapperClass(Map.class);\n",
    "    conf.setCombinerClass(Reduce.class);\n",
    "    conf.setReducerClass(Reduce.class);\n",
    "\n",
    "    conf.setInputFormat(TextInputFormat.class);\n",
    "    conf.setOutputFormat(TextOutputFormat.class);\n",
    "\n",
    "    FileInputFormat.setInputPaths(conf, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(conf, new Path(args[1]));\n",
    "\n",
    "    JobClient.runJob(conf);\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example program must be given as the first argument.\r\n",
      "Valid program names are:\r\n",
      "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\r\n",
      "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\r\n",
      "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\r\n",
      "  dbcount: An example job that count the pageview counts from a database.\r\n",
      "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\r\n",
      "  grep: A map/reduce program that counts the matches of a regex in the input.\r\n",
      "  join: A job that effects a join over sorted, equally partitioned datasets\r\n",
      "  multifilewc: A job that counts words from several files.\r\n",
      "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\r\n",
      "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\r\n",
      "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\r\n",
      "  randomwriter: A map/reduce program that writes 10GB of random data per node.\r\n",
      "  secondarysort: An example defining a secondary sort to the reduce.\r\n",
      "  sort: A map/reduce program that sorts the data written by the random writer.\r\n",
      "  sudoku: A sudoku solver.\r\n",
      "  teragen: Generate data for the terasort\r\n",
      "  terasort: Run the terasort\r\n",
      "  teravalidate: Checking results of terasort\r\n",
      "  wordcount: A map/reduce program that counts the words in the input files.\r\n",
      "  wordmean: A map/reduce program that counts the average length of the words in the input files.\r\n",
      "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\r\n",
      "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\r\n"
     ]
    }
   ],
   "source": [
    "# Hadoop available examples\n",
    "! hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wordcount <in> [<in>...] <out>\r\n"
     ]
    }
   ],
   "source": [
    "# Test wordcount\n",
    "! hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/10/12 11:55:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "15/10/12 11:56:02 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/root/.staging/job_1444641399800_0001\n",
      "org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://ipyhadoop:9000/user/root/dft\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:321)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:264)\n",
      "\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:385)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:597)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:614)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:492)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)\n",
      "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1314)\n",
      "\tat org.apache.hadoop.examples.WordCount.main(WordCount.java:87)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n",
      "\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n",
      "\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n"
     ]
    }
   ],
   "source": [
    "# Test wordcount\n",
    "! hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount dft dft-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ hadoop jar hadoop-examples.jar wordcount dft dft-output\n",
    "\n",
    "10/03/16 11:40:51 INFO mapred.FileInputFormat: Total input paths to process : 1 10/03/16 11:40:51 INFO mapred.JobClient: Running job: job_201003161102_0002 10/03/16 11:40:52 INFO mapred.JobClient: map 0% reduce 0%\n",
    "10/03/16 11:40:55 INFO mapred.JobClient: map 9% reduce 0% 10/03/16 11:40:56 INFO mapred.JobClient: map 27% reduce 0%\n",
    "10/03/16 11:40:58 INFO mapred.JobClient: map 45% reduce 0% 10/03/16 11:40:59 INFO mapred.JobClient: map 81% reduce 0% \n",
    "\n",
    "10/03/16 11:41:01 INFO mapred.JobClient: map 100% reduce 0% 10/03/16 11:41:09 INFO mapred.JobClient: Job complete: job_201003161102_0002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ hadoop dfs -cat dft-output/part-00000 | less \n",
    "\n",
    "“Information\" 1 \n",
    "\"J\" 1 \n",
    "“Plain\" 2 \n",
    "“Project\" 5 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGE**: cutting sandwiches with hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Write a Java class to count letters inside a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*...just kidding!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: myfile=/data/worker/ngs.sam\n"
     ]
    }
   ],
   "source": [
    "myfile = '/data/worker/ngs.sam'\n",
    "%env myfile $myfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded\n",
      "decompressed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget -q \"http://bit.ly/ngs_sample_data\" -O $myfile.bz2 && echo \"downloaded\"\n",
    "bunzip2 $myfile.bz2 && echo \"decompressed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "head -2000 data/ngs/input.sam | tail -n 10 \\\n",
    "| awk '{ print $3\":\"$4 }' | sort | uniq -c\n",
    "\n",
    "      3 chr1:142803456\n",
    "      3 chr1:142803458\n",
    "      1 chr1:142803465\n",
    "      1 chr1:142803470\n",
    "      2 chr1:142803471\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3 chr1:142803456\n",
      "      3 chr1:142803458\n",
      "      1 chr1:142803465\n",
      "      1 chr1:142803470\n",
      "      2 chr1:142803471\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `head -2000 data/ngs/input.sam | tail -n 10`\n",
    "\n",
    "1. `awk '{ print $3\":\"$4 }’`\n",
    "\n",
    "1. `sort`\n",
    "\n",
    "1. `uniq -c`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUT STREAM**\n",
    "`head -2000 data/ngs/input.sam | tail -n 10`\n",
    "\n",
    "**MAPPER**\n",
    "`awk '{ print $3\":\"$4 }'`\n",
    "\n",
    "**SHUFFLE**\n",
    "`sort`\n",
    "\n",
    "**REDUCER**\n",
    "`uniq -c`\n",
    "\n",
    "**OUTPUT STREAM**\n",
    "`<STDOUT>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# head -2000 data/ngs/input.sam | tail -n 10\n",
    "HSCAN:421:C47DAACXX:3:1206:5660:99605\t99\tchr1\t142803456\t10\t75M\t=\t142803517\t136\tCTGTGCATTCTTATGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATA\t@@CDDDD?DBFHFHIHIJJJJGHEIFHIIIHGDFEGEIAHAHGIIGIIIC?CBFCGHFCEG?@DGIIIGHIGGHC\tX0:i:3\tX1:i:1\tXA:Z:chr21,+9746045,75M,0;chr1,+143355186,75M,0;chr1,-143233123,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:0\tXM:i:0\tXO:i:0\tMQ:i:18\tXT:A:R\n",
    "\n",
    "[…]\n",
    "\n",
    "# head -2000 data/ngs/input.sam | tail -n 10 | awk '{ print $3\":\"$4 }'\n",
    "chr1:142803471\n",
    "chr1:142803456\n",
    "chr1:142803465\n",
    "chr1:142803458\n",
    "chr1:142803456\n",
    "chr1:142803471\n",
    "chr1:142803458\n",
    "chr1:142803456\n",
    "chr1:142803470\n",
    "chr1:142803458\n",
    "\n",
    "# head -2000 data/ngs/input.sam | tail -n 10 | awk '{ print $3\":\"$4 }' | sort\n",
    "chr1:142803456\n",
    "chr1:142803456\n",
    "chr1:142803456\n",
    "chr1:142803458\n",
    "chr1:142803458\n",
    "chr1:142803458\n",
    "chr1:142803465\n",
    "chr1:142803470\n",
    "chr1:142803471\n",
    "chr1:142803471\n",
    "\n",
    "# head -2000 data/ngs/input.sam | tail -n 10 | awk '{ print $3\":\"$4 }' | sort | uniq -c\n",
    "      3 chr1:142803456\n",
    "      3 chr1:142803458\n",
    "      1 chr1:142803465\n",
    "      1 chr1:142803470\n",
    "      2 chr1:142803471\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations with bash pipes as simulation of MapReduce\n",
    "\n",
    "* Serial steps\n",
    "* No file distribution\n",
    "* Single node\n",
    "* Single mapper\n",
    "* Single reducer\n",
    "* Can we add a Combiner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop streaming\n",
    "### Concepts and mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop streaming is a utility \n",
    "comes with the Hadoop distribution\n",
    "Allows to create and run Map/Reduce jobs \n",
    "with any executable or script as the mapper and/or the reducer\n",
    "Protocol steps:\n",
    "Create a Map/Reduce job\n",
    "Submit the job to an appropriate cluster\n",
    "Monitor the progress of the job until it completes\n",
    "Links to Hadoop HDFS job directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why?\n",
    "One of the unappetizing aspects of Hadoop to users of traditional HPC is that it is written in Java. \n",
    "Java is not originally designed to be a high-performance language\n",
    "Learning it is not easy for domain scientists\n",
    " Hadoop allows you to write map/reduce code in any language you want using the Hadoop Streaming interface\n",
    "It means turning an existing Python or Perl script into a Hadoop job\n",
    "Does not require learning any Java at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce with binaries/executables\n",
    "Executable is specified for mappers and reducers\n",
    "each mapper task will launch the executable as a separate process \n",
    "converts inputs into lines and feed to the stdin of the process\n",
    "the mapper collects the line oriented outputs from the stdout of the process \n",
    "converts each line into a key/value pair\n",
    "By default, the prefix of a line up to the first tab character is the key and the rest of the line (excluding the tab character) will be the value\n",
    "e.g. ”this is the key\\tvalue is the rest\\n”\n",
    "If there is no tab character in the line, then entire line is considered as key and the value is null (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A streaming command line example:\n",
    "``` bash\n",
    "$ hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -input myInputDirs \\\n",
    "    -output myOutputDir \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer /bin/wc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A streaming command line example **for python**:\n",
    "``` bash\n",
    "$ hadoop jar hadoop-streaming-1.2.1.jar \\\n",
    "    -input input_dir/ \\\n",
    "    -output output_dir/ \\\n",
    "    -mapper mapper.py \\\n",
    "    -file mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -file reducer.py \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting the Hadoop Streaming job:\n",
    "\n",
    "* Make sure your scripts have no errors\n",
    "* Do mapper and reducer scripts actually work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a matter of running them through pipes on a **little bit** of sample data,\n",
    "\n",
    "like `cat` or `head` linux bash commands, with pipes, as seen before.\n",
    "\n",
    "```\n",
    "# Simulating hadoop streaming with bash pipes\n",
    "$ cat $file | python mapper.py | sort | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First approach: split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    pieces = line.split('\\t')\n",
    "    print(pieces) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@HD', 'VN:1.4', 'GO:none', 'SO:coordinate']\r\n",
      "['@SQ', 'SN:chrM', 'LN:16571']\r\n",
      "['@SQ', 'SN:chr1', 'LN:249250621']\r\n",
      "['@SQ', 'SN:chr2', 'LN:243199373']\r\n",
      "['@SQ', 'SN:chr3', 'LN:198022430']\r\n",
      "['@SQ', 'SN:chr4', 'LN:191154276']\r\n",
      "['@SQ', 'SN:chr5', 'LN:180915260']\r\n",
      "['@SQ', 'SN:chr6', 'LN:171115067']\r\n",
      "['@SQ', 'SN:chr7', 'LN:159138663']\r\n",
      "['@SQ', 'SN:chr8', 'LN:146364022']\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 10 $myfile | python mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "\n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "    print(mychr,mystart.__str__())\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrM 14\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 100 $myfile | python mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "    \n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "\n",
    "    mystop = mystart + len(myseq)\n",
    "\n",
    "    # Each element with coverage\n",
    "    for i in range(mystart,mystop):\n",
    "        results = [mychr+SEP+i.__str__(), \"1\"]\n",
    "        print(TAB.join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrM:14\t1\r\n",
      "chrM:15\t1\r\n",
      "chrM:16\t1\r\n",
      "chrM:17\t1\r\n",
      "chrM:18\t1\r\n",
      "chrM:19\t1\r\n",
      "chrM:20\t1\r\n",
      "chrM:21\t1\r\n",
      "chrM:22\t1\r\n",
      "chrM:23\t1\r\n",
      "chrM:24\t1\r\n",
      "chrM:25\t1\r\n",
      "chrM:26\t1\r\n",
      "chrM:27\t1\r\n",
      "chrM:28\t1\r\n",
      "chrM:29\t1\r\n",
      "chrM:30\t1\r\n",
      "chrM:31\t1\r\n",
      "chrM:32\t1\r\n",
      "chrM:33\t1\r\n",
      "chrM:34\t1\r\n",
      "chrM:35\t1\r\n",
      "chrM:36\t1\r\n",
      "chrM:37\t1\r\n",
      "chrM:38\t1\r\n",
      "chrM:39\t1\r\n",
      "chrM:40\t1\r\n",
      "chrM:41\t1\r\n",
      "chrM:42\t1\r\n",
      "chrM:43\t1\r\n",
      "chrM:44\t1\r\n",
      "chrM:45\t1\r\n",
      "chrM:46\t1\r\n",
      "chrM:47\t1\r\n",
      "chrM:48\t1\r\n",
      "chrM:49\t1\r\n",
      "chrM:50\t1\r\n",
      "chrM:51\t1\r\n",
      "chrM:52\t1\r\n",
      "chrM:53\t1\r\n",
      "chrM:54\t1\r\n",
      "chrM:55\t1\r\n",
      "chrM:56\t1\r\n",
      "chrM:57\t1\r\n",
      "chrM:58\t1\r\n",
      "chrM:59\t1\r\n",
      "chrM:60\t1\r\n",
      "chrM:61\t1\r\n",
      "chrM:62\t1\r\n",
      "chrM:63\t1\r\n",
      "chrM:64\t1\r\n",
      "chrM:65\t1\r\n",
      "chrM:66\t1\r\n",
      "chrM:67\t1\r\n",
      "chrM:68\t1\r\n",
      "chrM:69\t1\r\n",
      "chrM:70\t1\r\n",
      "chrM:71\t1\r\n",
      "chrM:72\t1\r\n",
      "chrM:73\t1\r\n",
      "chrM:74\t1\r\n",
      "chrM:75\t1\r\n",
      "chrM:76\t1\r\n",
      "chrM:77\t1\r\n",
      "chrM:78\t1\r\n",
      "chrM:79\t1\r\n",
      "chrM:80\t1\r\n",
      "chrM:81\t1\r\n",
      "chrM:82\t1\r\n",
      "chrM:83\t1\r\n",
      "chrM:84\t1\r\n",
      "chrM:85\t1\r\n",
      "chrM:86\t1\r\n",
      "chrM:87\t1\r\n",
      "chrM:88\t1\r\n",
      "chrM:14\t1\r\n",
      "chrM:15\t1\r\n",
      "chrM:16\t1\r\n",
      "chrM:17\t1\r\n",
      "chrM:18\t1\r\n",
      "chrM:19\t1\r\n",
      "chrM:20\t1\r\n",
      "chrM:21\t1\r\n",
      "chrM:22\t1\r\n",
      "chrM:23\t1\r\n",
      "chrM:24\t1\r\n",
      "chrM:25\t1\r\n",
      "chrM:26\t1\r\n",
      "chrM:27\t1\r\n",
      "chrM:28\t1\r\n",
      "chrM:29\t1\r\n",
      "chrM:30\t1\r\n",
      "chrM:31\t1\r\n",
      "chrM:32\t1\r\n",
      "chrM:33\t1\r\n",
      "chrM:34\t1\r\n",
      "chrM:35\t1\r\n",
      "chrM:36\t1\r\n",
      "chrM:37\t1\r\n",
      "chrM:38\t1\r\n",
      "chrM:39\t1\r\n",
      "chrM:40\t1\r\n",
      "chrM:41\t1\r\n",
      "chrM:42\t1\r\n",
      "chrM:43\t1\r\n",
      "chrM:44\t1\r\n",
      "chrM:45\t1\r\n",
      "chrM:46\t1\r\n",
      "chrM:47\t1\r\n",
      "chrM:48\t1\r\n",
      "chrM:49\t1\r\n",
      "chrM:50\t1\r\n",
      "chrM:51\t1\r\n",
      "chrM:52\t1\r\n",
      "chrM:53\t1\r\n",
      "chrM:54\t1\r\n",
      "chrM:55\t1\r\n",
      "chrM:56\t1\r\n",
      "chrM:57\t1\r\n",
      "chrM:58\t1\r\n",
      "chrM:59\t1\r\n",
      "chrM:60\t1\r\n",
      "chrM:61\t1\r\n",
      "chrM:62\t1\r\n",
      "chrM:63\t1\r\n",
      "chrM:64\t1\r\n",
      "chrM:65\t1\r\n",
      "chrM:66\t1\r\n",
      "chrM:67\t1\r\n",
      "chrM:68\t1\r\n",
      "chrM:69\t1\r\n",
      "chrM:70\t1\r\n",
      "chrM:71\t1\r\n",
      "chrM:72\t1\r\n",
      "chrM:73\t1\r\n",
      "chrM:74\t1\r\n",
      "chrM:75\t1\r\n",
      "chrM:76\t1\r\n",
      "chrM:77\t1\r\n",
      "chrM:78\t1\r\n",
      "chrM:79\t1\r\n",
      "chrM:80\t1\r\n",
      "chrM:81\t1\r\n",
      "chrM:82\t1\r\n",
      "chrM:83\t1\r\n",
      "chrM:84\t1\r\n",
      "chrM:85\t1\r\n",
      "chrM:86\t1\r\n",
      "chrM:87\t1\r\n",
      "chrM:88\t1\r\n",
      "chrM:19\t1\r\n",
      "chrM:20\t1\r\n",
      "chrM:21\t1\r\n",
      "chrM:22\t1\r\n",
      "chrM:23\t1\r\n",
      "chrM:24\t1\r\n",
      "chrM:25\t1\r\n",
      "chrM:26\t1\r\n",
      "chrM:27\t1\r\n",
      "chrM:28\t1\r\n",
      "chrM:29\t1\r\n",
      "chrM:30\t1\r\n",
      "chrM:31\t1\r\n",
      "chrM:32\t1\r\n",
      "chrM:33\t1\r\n",
      "chrM:34\t1\r\n",
      "chrM:35\t1\r\n",
      "chrM:36\t1\r\n",
      "chrM:37\t1\r\n",
      "chrM:38\t1\r\n",
      "chrM:39\t1\r\n",
      "chrM:40\t1\r\n",
      "chrM:41\t1\r\n",
      "chrM:42\t1\r\n",
      "chrM:43\t1\r\n",
      "chrM:44\t1\r\n",
      "chrM:45\t1\r\n",
      "chrM:46\t1\r\n",
      "chrM:47\t1\r\n",
      "chrM:48\t1\r\n",
      "chrM:49\t1\r\n",
      "chrM:50\t1\r\n",
      "chrM:51\t1\r\n",
      "chrM:52\t1\r\n",
      "chrM:53\t1\r\n",
      "chrM:54\t1\r\n",
      "chrM:55\t1\r\n",
      "chrM:56\t1\r\n",
      "chrM:57\t1\r\n",
      "chrM:58\t1\r\n",
      "chrM:59\t1\r\n",
      "chrM:60\t1\r\n",
      "chrM:61\t1\r\n",
      "chrM:62\t1\r\n",
      "chrM:63\t1\r\n",
      "chrM:64\t1\r\n",
      "chrM:65\t1\r\n",
      "chrM:66\t1\r\n",
      "chrM:67\t1\r\n",
      "chrM:68\t1\r\n",
      "chrM:69\t1\r\n",
      "chrM:70\t1\r\n",
      "chrM:71\t1\r\n",
      "chrM:72\t1\r\n",
      "chrM:73\t1\r\n",
      "chrM:74\t1\r\n",
      "chrM:75\t1\r\n",
      "chrM:76\t1\r\n",
      "chrM:77\t1\r\n",
      "chrM:78\t1\r\n",
      "chrM:79\t1\r\n",
      "chrM:80\t1\r\n",
      "chrM:81\t1\r\n",
      "chrM:82\t1\r\n",
      "chrM:83\t1\r\n",
      "chrM:84\t1\r\n",
      "chrM:85\t1\r\n",
      "chrM:86\t1\r\n",
      "chrM:87\t1\r\n",
      "chrM:88\t1\r\n",
      "chrM:89\t1\r\n",
      "chrM:90\t1\r\n",
      "chrM:91\t1\r\n",
      "chrM:92\t1\r\n",
      "chrM:93\t1\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 100 $myfile | python mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle step \n",
    "\n",
    "A lot happens, transparent to the developer\n",
    "Mappers’s output is transformed and distributed to the reducers\n",
    "All key/value pairs are sorted before sent to reducer function\n",
    "Pairs sharing the same key are sent to the same reducer\n",
    "If you encounter a key that is different from the last key you processed, you know that previous key will never appear again\n",
    "If your keys are all the same\n",
    "only use one reducer and gain no parallelization\n",
    "come up with a more unique key if this happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "last_value = \"\"\n",
    "value_count = 1\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    value, count = line.split(TAB)\n",
    "    count = int(count)\n",
    "\n",
    "    # if this is the first iteration\n",
    "    if not last_value:\n",
    "        last_value = value\n",
    "\n",
    "    # if they're the same, log it\n",
    "    if value == last_value:\n",
    "        value_count += count\n",
    "    else:\n",
    "        # state change\n",
    "        try: \n",
    "            print(TAB.join([last_value, str(value_count)]))\n",
    "        except:\n",
    "            pass\n",
    "        last_value = value\n",
    "        value_count = 1\n",
    "\n",
    "# LAST ONE after all records have been received\n",
    "print(TAB.join([last_value, str(value_count)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1:10000\t3\n",
      "chr1:10001\t2\n",
      "chr1:10002\t2\n",
      "chr1:10003\t2\n",
      "chr1:10004\t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m5.921s\n",
      "user\t0m5.340s\n",
      "sys\t0m0.150s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# needs ~ 10 seconds for running\n",
    "time head -n 10000 worker/ngs.sam | python mapper.py | sort | python reducer.py | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t3m30.950s\n",
      "user\t3m8.630s\n",
      "sys\t0m10.660s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "time cat worker/ngs.sam | python mapper.py | sort | python reducer.py > worker/out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Switching to real Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A working python code tested on pipes should work with Hadoop Streaming\n",
    "\n",
    "To make this work we need to handle copy of input and output file \n",
    "inside the Hadoop FS\n",
    "Also the job tracker logs will be found inside HDFS\n",
    "We are going to build a bash script to make our workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HDFS commands to interact with Hadoop file system\n",
    "\n",
    "Create dir \n",
    "hadoop fs -mkdir\n",
    "Copy file\n",
    "hadoop fs -put\n",
    "Check if file is there\n",
    "hadoop fs -ls\n",
    "Remove recursively data\n",
    "hadoop fs -rmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hadoop Streaming needs “binaries” to execute\n",
    "\n",
    "You need to specify interpreter inside the script\n",
    "#!/usr/bin/env python\n",
    "Make the script executable\n",
    "chmod +x hs*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final launch via bash command for using Hadoop streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts on Hadoop Streaming \n",
    "\n",
    "Provides options to write MapReduce jobs in other languages\n",
    "One of the best examples of flexibility available to MapReduce\n",
    "Fast\n",
    "Simple\n",
    "Close to the original standard Java API power\n",
    "\n",
    "Even executables can be used to work as a MapReduce job (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where it really works\n",
    "\n",
    "When the developer do not have knowhow of Java \n",
    "Write Mapper/Reducer in any scripting language \n",
    "Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons\n",
    "\n",
    "Force scripts in a Java VM\n",
    "Although free overhead\n",
    "The program/executable should be able to take input from STDIN and produce output at STDOUT\n",
    "Restrictions on the input/output formats\n",
    "Does not take care of input and output file and directory preparation\n",
    "User have to implement hdfs commands “hand-made”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where it falls short\n",
    "\n",
    "No pythonic way to work the MapReduce code\n",
    "Because it was not written specifically for python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap \n",
    "\n",
    "Hadoop streaming handles Hadoop in almost a classic manner\n",
    "Wrap any executable (and script)\n",
    "Wrap python scripts\n",
    "Runnable on a cluster\n",
    "using a non-interactive, all-encapsulated job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Docker lightweight virtualization\n",
    "* Two environments \n",
    "* Schema"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
