{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop\n",
    "(and Hadoop streaming)\n",
    "\n",
    "<center>\n",
    "<img src='http://www.datameer.com/images/technology/hadoop-pic1.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**MapReduce** is a completely different paradigm \n",
    "\n",
    "* Solving a certain subset of parallelizable problems \n",
    "    - around the bottleneck of ingesting input data from disk\n",
    "* Traditional parallelism brings the data to the computing machine\n",
    "    - Map/reduce does the opposite, it brings the compute to the data\n",
    "* Input data is not stored on a separate storage system\n",
    "* Data exists in little pieces \n",
    "    - and is permanently stored on each computing node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MapReduce is the programming paradigm that allows massive scalability across thousands of servers.\n",
    "\n",
    "Its open source server implementation is the *Hadoop* cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Also always keep in mind that ***HDFS*** is fundamental to Hadoop \n",
    "\n",
    "* it provides the data chunking distribution across compute elements \n",
    "* necessary for map/reduce applications to be efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word count\n",
    "The '`Hello World`' for MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Among the simplest of full Hadoop jobs you can run\n",
    "\n",
    "<img src='http://www.glennklockwood.com/data-intensive/hadoop/wordcount-schematic.png'\n",
    "width='700'>\n",
    "<small>Reading ***Moby Dick*** </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works\n",
    "* The **MAP step** will take the raw text and convert it to key/value pairs\n",
    "    - Each key is a word\n",
    "    - All keys (words) will have a value of 1\n",
    "\n",
    "\n",
    "* The **REDUCE step** will combine all duplicate keys \n",
    "    - By adding up their values (sum)\n",
    "    - Every key (word) has a value of 1 (Map)\n",
    "    - Output is reduced to a list of unique keys\n",
    "    - Each key’s value corresponding to key's (word's) count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='http://disco.readthedocs.org/en/latest/_images/map_shuffle_reduce.png' width=800>\n",
    "</center>\n",
    "\n",
    "### Map function:\n",
    "processes data and generates a set of  intermediate key/value pairs.\n",
    "\n",
    "\n",
    "### Reduce function:\n",
    "merges all intermediate values  associated with the same intermediate key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A WordCount example \n",
    "*(with Java)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider doing a word count of the following file using  MapReduce:\n",
    "```\n",
    "Hello World Bye World\n",
    "Hello Hadoop Goodbye Hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The map function reads in words one at a time outputs (“word”, 1) for each parsed input word\n",
    "\n",
    "```\n",
    "(Hello, 1)\n",
    "(World, 1)\n",
    "(Bye, 1)\n",
    "(World, 1)\n",
    "(Hello, 1)\n",
    "(Hadoop, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The shuffle phase between map and reduce creates a  list of values associated with each key\n",
    "```\n",
    "(Bye, (1))\n",
    "(Goodbye, (1))\n",
    "(Hadoop, (1, 1))\n",
    "(Hello, (1, 1))\n",
    "(World, (1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce function sums the numbers in the list for each  key and outputs (word, count) pairs\n",
    "```\n",
    "(Bye, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 2)\n",
    "(Hello, 2)\n",
    "(World, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can you do this with Java?\n",
    "(the Hadoop framework native language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "// Imports\n",
    "package org.myorg;\n",
    "import java.io.IOException;\n",
    "import java.util.*;\n",
    "import org.apache.hadoop.*\n",
    "\n",
    "// Create JAVA class\n",
    "public class WordCount {\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Mapper function\n",
    "  public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      String line = value.toString();\n",
    "      StringTokenizer tokenizer = new StringTokenizer(line);\n",
    "      while (tokenizer.hasMoreTokens()) {\n",
    "        word.set(tokenizer.nextToken());\n",
    "        output.collect(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Reducer function\n",
    "  public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      int sum = 0;\n",
    "      while (values.hasNext()) {\n",
    "        sum += values.next().get();\n",
    "      }\n",
    "      output.collect(key, new IntWritable(sum));\n",
    "    }\n",
    "  }\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "``` Java\n",
    "//Main function\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    JobConf conf = new JobConf(WordCount.class);\n",
    "    conf.setJobName(\"wordcount\");\n",
    "\n",
    "    conf.setOutputKeyClass(Text.class);\n",
    "    conf.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    conf.setMapperClass(Map.class);\n",
    "    conf.setCombinerClass(Reduce.class);\n",
    "    conf.setReducerClass(Reduce.class);\n",
    "\n",
    "    conf.setInputFormat(TextInputFormat.class);\n",
    "    conf.setOutputFormat(TextOutputFormat.class);\n",
    "\n",
    "    FileInputFormat.setInputPaths(conf, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(conf, new Path(args[1]));\n",
    "\n",
    "    JobClient.runJob(conf);\n",
    "  }\n",
    "```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can test the Java code here. *Live*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: `myinput': File exists\n",
      "put: `myinput/file01': File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Preprocess with HDFS\n",
    "\n",
    "# Create input directory\n",
    "hdfs dfs -mkdir myinput\n",
    "# Save one file inside\n",
    "hdfs dfs -put /data/worker/books/twolines.txt myinput/file01\n",
    "# Remove output or Hadoop will give error if existing\n",
    "hdfs dfs -rm -r -f myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_EXAMPLE=/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar\n"
     ]
    }
   ],
   "source": [
    "# Save this long string inside the environment\n",
    "HADOOP_PREFIX = %env HADOOP_PREFIX\n",
    "%env HADOOP_EXAMPLE $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\r\n",
      "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\r\n",
      "  multifilewc: A job that counts words from several files.\r\n",
      "  wordcount: A map/reduce program that counts the words in the input files.\r\n",
      "  wordmean: A map/reduce program that counts the average length of the words in the input files.\r\n",
      "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\r\n",
      "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\r\n"
     ]
    }
   ],
   "source": [
    "# Hadoop available examples\n",
    "! hadoop jar $HADOOP_EXAMPLE | grep word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wordcount <in> [<in>...] <out>\r\n"
     ]
    }
   ],
   "source": [
    "# Check wordcount\n",
    "! hadoop jar $HADOOP_EXAMPLE wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/10/12 16:24:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://ipyhadoop:9000/user/root/myoutput already exists\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:146)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:562)\n",
      "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:432)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)\n",
      "\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n",
      "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)\n",
      "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1314)\n",
      "\tat org.apache.hadoop.examples.WordCount.main(WordCount.java:87)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n",
      "\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n",
      "\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:606)\n",
      "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n",
      "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n"
     ]
    }
   ],
   "source": [
    "# Test wordcount with real hadoop on our system\n",
    "! hadoop jar $HADOOP_EXAMPLE wordcount myinput myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World Bye World\r\n",
      "Hello Hadoop Goodbye Hadoop"
     ]
    }
   ],
   "source": [
    "# This was our input\n",
    "! cat /data/worker/books/twolines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye\t1\r\n",
      "Goodbye\t1\r\n",
      "Hadoop\t2\r\n",
      "Hello\t2\r\n",
      "World\t2\r\n"
     ]
    }
   ],
   "source": [
    "# This is our output\n",
    "! hadoop fs -cat myoutput/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "<img src='https://pbs.twimg.com/media/B2RlCy-IIAEFCLC.jpg' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Write your first Java class to count letters inside a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*...just kidding!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop like `pipes` in Unix Bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prepare a data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: myfile=/data/worker/ngs.sam\n"
     ]
    }
   ],
   "source": [
    "# Variables for python and bash\n",
    "myfile = '/data/worker/ngs.sam'\n",
    "%env myfile $myfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded\n",
      "decompressed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Download compressed NGS data from a link\n",
    "wget -q \"http://bit.ly/ngs_sample_data\" -O $myfile.bz2 && echo \"downloaded\"\n",
    "# Decompress the file\n",
    "bunzip2 $myfile.bz2 && echo \"decompressed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3 chr1:142803456\n",
      "      3 chr1:142803458\n",
      "      1 chr1:142803465\n",
      "      1 chr1:142803470\n",
      "      2 chr1:142803471\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Bash piping our own MapReduce with Unix commands\n",
    "head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Understanding better\n",
    "\n",
    "Splitting the command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. `head -2000 data/ngs/input.sam | tail -n 10`\n",
    "\n",
    "1. `awk '{ print $3\":\"$4 }’`\n",
    "\n",
    "1. `sort`\n",
    "\n",
    "1. `uniq -c`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**INPUT STREAM**\n",
    "`head -2000 data/ngs/input.sam | tail -n 10`\n",
    "\n",
    "**MAPPER**\n",
    "`awk '{ print $3\":\"$4 }'`\n",
    "\n",
    "**SHUFFLE**\n",
    "`sort`\n",
    "\n",
    "**REDUCER**\n",
    "`uniq -c`\n",
    "\n",
    "**OUTPUT STREAM**\n",
    "`<STDOUT>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see step by step what it's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSCAN:421:C47DAACXX:3:1206:5660:99605\t99\tchr1\t142803456\t10\t75M\t=\t142803517\t136\tCTGTGCATTCTTATGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATA\t@@CDDDD?DBFHFHIHIJJJJGHEIFHIIIHGDFEGEIAHAHGIIGIIIC?CBFCGHFCEG?@DGIIIGHIGGHC\tX0:i:3\tX1:i:1\tXA:Z:chr21,+9746045,75M,0;chr1,+143355186,75M,0;chr1,-143233123,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:0\tXM:i:0\tXO:i:0\tMQ:i:18\tXT:A:R\r\n",
      "HSCAN:421:C47DAACXX:3:1207:14092:152623\t99\tchr1\t142803456\t0\t75M\t=\t142803560\t180\tCTGTGCATTCTTATGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATA\tCCCDFFFFHHHHGJJIJJJJJIJJJJJJJHIJJJJJIJJIJJJJJJJJIIIFHHEHJJIGHHIIJJJIIIJIIJG\tX0:i:3\tX1:i:1\tXA:Z:chr21,+9746045,75M,0;chr1,+143355186,75M,0;chr1,-143233123,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:0\tXM:i:0\tXO:i:0\tMQ:i:0\tXT:A:R\r\n",
      "HSCAN:421:C47DAACXX:3:1301:4054:177529\t99\tchr1\t142803456\t12\t75M\t=\t142803494\t113\tCTGTGCATTCTTATGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATA\tCCCFFFFFHHHHHJJHHJJIIGJJJJJJJIIGGIJJJJJJJJHIJJIIHIFFHIIJJJJJIGIJJHJEHHIGIIJ\tX0:i:3\tX1:i:1\tXA:Z:chr21,+9746045,75M,0;chr1,+143355186,75M,0;chr1,-143233123,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:0\tXM:i:0\tXO:i:0\tMQ:i:20\tXT:A:R\r\n",
      "HSCAN:421:C47DAACXX:2:1302:3204:111307\t99\tchr1\t142803458\t12\t75M\t=\t142803484\t101\tGTGCATTCTTATGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATAGA\tBCCFFFFFHHHHGCGIJIHHIHHIJJJJJJJIJIJIJJJJJJJJIJEHHHC<DHIIJJJIGIJIIJJJJJCHGE4\tX0:i:3\tX1:i:1\tXA:Z:chr21,+9746047,75M,0;chr1,+143355188,75M,0;chr1,-143233121,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:0\tXM:i:0\tXO:i:0\tMQ:i:20\tXT:A:R\r\n",
      "HSCAN:421:C47DAACXX:4:1103:18240:14393\t99\tchr1\t142803458\t0\t75M\t=\t142803580\t197\tGTGCATTCTTATGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATAGA\tCBCFFFFFHHHHGIJIJJJJJIHJJJJJJJJJIJJJJJJJIIGGIGHIEGFHIJJIJJIIIJJJJIIIIIIIJJ@\tX0:i:3\tX1:i:1\tXA:Z:chr21,+9746047,75M,0;chr1,+143355188,75M,0;chr1,-143233121,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:0\tXM:i:0\tXO:i:0\tMQ:i:0\tXT:A:R\r\n",
      "HSCAN:421:C47DAACXX:4:1107:18819:78768\t99\tchr1\t142803458\t12\t75M\t=\t142803495\t112\tGTGCATTCTTATGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATAGA\tB?BFDFFDHFFHHFGGIGJIIGIJHIIHGIIIGJIIJIJJJIIIJIJFEHGFHJEIJIGIIJIIJJJJIIJGGI:\tX0:i:3\tX1:i:1\tXA:Z:chr21,+9746047,75M,0;chr1,+143355188,75M,0;chr1,-143233121,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:0\tXM:i:0\tXO:i:0\tMQ:i:20\tXT:A:R\r\n",
      "HSCAN:421:C47DAACXX:3:1107:14197:140375\t99\tchr1\t142803465\t17\t75M\t=\t142803533\t143\tCTTATGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATAGATGTTGGA\t@@@BBD?DHHHABGE@FEHIFGHGHFHIIIIIICGHHIEIIHHIIH9FBGDFHGIIIIIIIIIIGHGEHIICGCF\tX0:i:3\tX1:i:1\tXA:Z:chr21,+9746054,75M,0;chr1,+143355195,75M,0;chr1,-143233114,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:0\tXM:i:0\tXO:i:0\tMQ:i:18\tXT:A:R\r\n",
      "HSCAN:421:C47DAACXX:4:1101:8419:153036\t99\tchr1\t142803470\t20\t75M\t=\t142803563\t168\tGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATAGATGTTGGAATTCC\tB@BFFFFFHHHHHJIGGJIJJJJIJJJJIJIJJJIGJJIIJJJIIJIIJJHIGGGJIJJCHEHIGDGIGIIIEHG\tX0:i:1\tX1:i:2\tXA:Z:chr21,+9746059,75M,1;chr1,+143355200,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:20\tXM:i:0\tXO:i:0\tMQ:i:12\tXT:A:U\r\n",
      "HSCAN:421:C47DAACXX:1:1207:3548:56254\t163\tchr1\t142803471\t38\t75M\t=\t142803533\t137\tATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATAGATGTTGGAATTCCT\tBCCFFFEFHHHHHJJJJIJIJHJJGJIJJJJJJIHIHIJIIJJJJJHHGIHHIIIIJHJHIGIIJJJJEGGIJGH\tX0:i:1\tX1:i:2\tXA:Z:chr21,+9746060,75M,1;chr1,+143355201,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:18\tNM:i:0\tSM:i:20\tXM:i:0\tXO:i:0\tMQ:i:38\tXT:A:U\r\n",
      "HSCAN:421:C47DAACXX:4:1305:1710:37950\t99\tchr1\t142803471\t20\t75M\t=\t142803553\t157\tATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATAGATGTTGGAATTCCT\tCCCFFFFFHHHHGJIIJJJJJJJJIJJIJIJJJGIGIIJJJJJIIJEFHIGHGIJJIBHGIEEGGIIGGIGIGG@\tX0:i:1\tX1:i:2\tXA:Z:chr21,+9746060,75M,1;chr1,+143355201,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:20\tXM:i:0\tXO:i:0\tMQ:i:12\tXT:A:U\r\n"
     ]
    }
   ],
   "source": [
    "# STREAMING THE FILE\n",
    "! head -2000 $myfile | tail -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1:142803456\r\n",
      "chr1:142803456\r\n",
      "chr1:142803456\r\n",
      "chr1:142803458\r\n",
      "chr1:142803458\r\n",
      "chr1:142803458\r\n",
      "chr1:142803465\r\n",
      "chr1:142803470\r\n",
      "chr1:142803471\r\n",
      "chr1:142803471\r\n"
     ]
    }
   ],
   "source": [
    "# MAPPING\n",
    "! head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1:142803456\r\n",
      "chr1:142803456\r\n",
      "chr1:142803456\r\n",
      "chr1:142803458\r\n",
      "chr1:142803458\r\n",
      "chr1:142803458\r\n",
      "chr1:142803465\r\n",
      "chr1:142803470\r\n",
      "chr1:142803471\r\n",
      "chr1:142803471\r\n"
     ]
    }
   ],
   "source": [
    "# SHUFFLING\n",
    "! head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }' | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3 chr1:142803456\r\n",
      "      3 chr1:142803458\r\n",
      "      1 chr1:142803465\r\n",
      "      1 chr1:142803470\r\n",
      "      2 chr1:142803471\r\n"
     ]
    }
   ],
   "source": [
    "# REDUCER\n",
    "! head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "<br>\n",
    "\n",
    "<big>Play *a little bit* with bash pipes to understand how MapReduce process data</big>\n",
    "\n",
    "<small>\n",
    "Note: You may do bash commands inside the notebook or via bash.\n",
    "\n",
    "To access a shell in your docker container just run **in your host terminal**:\n",
    "\n",
    "```\n",
    "docker exec -it $(docker ps | grep client_ | awk '{print $1}') bash\n",
    "\n",
    "```\n",
    "</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Considerations with bash pipes as simulation of MapReduce\n",
    "\n",
    "* Serial steps\n",
    "* No file distribution\n",
    "* Single node\n",
    "* Single mapper\n",
    "* Single reducer\n",
    "* Can we add a Combiner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop streaming\n",
    "### Concepts and mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop streaming is a utility \n",
    "comes with the Hadoop distribution\n",
    "Allows to create and run Map/Reduce jobs \n",
    "with any executable or script as the mapper and/or the reducer\n",
    "Protocol steps:\n",
    "Create a Map/Reduce job\n",
    "Submit the job to an appropriate cluster\n",
    "Monitor the progress of the job until it completes\n",
    "Links to Hadoop HDFS job directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why?\n",
    "One of the unappetizing aspects of Hadoop to users of traditional HPC is that it is written in Java. \n",
    "Java is not originally designed to be a high-performance language\n",
    "Learning it is not easy for domain scientists\n",
    " Hadoop allows you to write map/reduce code in any language you want using the Hadoop Streaming interface\n",
    "It means turning an existing Python or Perl script into a Hadoop job\n",
    "Does not require learning any Java at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce with binaries/executables\n",
    "Executable is specified for mappers and reducers\n",
    "each mapper task will launch the executable as a separate process \n",
    "converts inputs into lines and feed to the stdin of the process\n",
    "the mapper collects the line oriented outputs from the stdout of the process \n",
    "converts each line into a key/value pair\n",
    "By default, the prefix of a line up to the first tab character is the key and the rest of the line (excluding the tab character) will be the value\n",
    "e.g. ”this is the key\\tvalue is the rest\\n”\n",
    "If there is no tab character in the line, then entire line is considered as key and the value is null (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A streaming command line example:\n",
    "``` bash\n",
    "$ hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -input myInputDirs \\\n",
    "    -output myOutputDir \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer /bin/wc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A streaming command line example **for python**:\n",
    "``` bash\n",
    "$ hadoop jar hadoop-streaming-1.2.1.jar \\\n",
    "    -input input_dir/ \\\n",
    "    -output output_dir/ \\\n",
    "    -mapper mapper.py \\\n",
    "    -file mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -file reducer.py \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting the Hadoop Streaming job:\n",
    "\n",
    "* Make sure your scripts have no errors\n",
    "* Do mapper and reducer scripts actually work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a matter of running them through pipes on a **little bit** of sample data,\n",
    "\n",
    "like `cat` or `head` linux bash commands, with pipes, as seen before.\n",
    "\n",
    "```\n",
    "# Simulating hadoop streaming with bash pipes\n",
    "$ cat $file | python mapper.py | sort | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First approach: split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    pieces = line.split('\\t')\n",
    "    print(pieces) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@HD', 'VN:1.4', 'GO:none', 'SO:coordinate']\r\n",
      "['@SQ', 'SN:chrM', 'LN:16571']\r\n",
      "['@SQ', 'SN:chr1', 'LN:249250621']\r\n",
      "['@SQ', 'SN:chr2', 'LN:243199373']\r\n",
      "['@SQ', 'SN:chr3', 'LN:198022430']\r\n",
      "['@SQ', 'SN:chr4', 'LN:191154276']\r\n",
      "['@SQ', 'SN:chr5', 'LN:180915260']\r\n",
      "['@SQ', 'SN:chr6', 'LN:171115067']\r\n",
      "['@SQ', 'SN:chr7', 'LN:159138663']\r\n",
      "['@SQ', 'SN:chr8', 'LN:146364022']\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 10 $myfile | python mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "\n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "    print(mychr,mystart.__str__())\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrM 14\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 100 $myfile | python mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "    \n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "\n",
    "    mystop = mystart + len(myseq)\n",
    "\n",
    "    # Each element with coverage\n",
    "    for i in range(mystart,mystop):\n",
    "        results = [mychr+SEP+i.__str__(), \"1\"]\n",
    "        print(TAB.join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrM:14\t1\r\n",
      "chrM:15\t1\r\n",
      "chrM:16\t1\r\n",
      "chrM:17\t1\r\n",
      "chrM:18\t1\r\n",
      "chrM:19\t1\r\n",
      "chrM:20\t1\r\n",
      "chrM:21\t1\r\n",
      "chrM:22\t1\r\n",
      "chrM:23\t1\r\n",
      "chrM:24\t1\r\n",
      "chrM:25\t1\r\n",
      "chrM:26\t1\r\n",
      "chrM:27\t1\r\n",
      "chrM:28\t1\r\n",
      "chrM:29\t1\r\n",
      "chrM:30\t1\r\n",
      "chrM:31\t1\r\n",
      "chrM:32\t1\r\n",
      "chrM:33\t1\r\n",
      "chrM:34\t1\r\n",
      "chrM:35\t1\r\n",
      "chrM:36\t1\r\n",
      "chrM:37\t1\r\n",
      "chrM:38\t1\r\n",
      "chrM:39\t1\r\n",
      "chrM:40\t1\r\n",
      "chrM:41\t1\r\n",
      "chrM:42\t1\r\n",
      "chrM:43\t1\r\n",
      "chrM:44\t1\r\n",
      "chrM:45\t1\r\n",
      "chrM:46\t1\r\n",
      "chrM:47\t1\r\n",
      "chrM:48\t1\r\n",
      "chrM:49\t1\r\n",
      "chrM:50\t1\r\n",
      "chrM:51\t1\r\n",
      "chrM:52\t1\r\n",
      "chrM:53\t1\r\n",
      "chrM:54\t1\r\n",
      "chrM:55\t1\r\n",
      "chrM:56\t1\r\n",
      "chrM:57\t1\r\n",
      "chrM:58\t1\r\n",
      "chrM:59\t1\r\n",
      "chrM:60\t1\r\n",
      "chrM:61\t1\r\n",
      "chrM:62\t1\r\n",
      "chrM:63\t1\r\n",
      "chrM:64\t1\r\n",
      "chrM:65\t1\r\n",
      "chrM:66\t1\r\n",
      "chrM:67\t1\r\n",
      "chrM:68\t1\r\n",
      "chrM:69\t1\r\n",
      "chrM:70\t1\r\n",
      "chrM:71\t1\r\n",
      "chrM:72\t1\r\n",
      "chrM:73\t1\r\n",
      "chrM:74\t1\r\n",
      "chrM:75\t1\r\n",
      "chrM:76\t1\r\n",
      "chrM:77\t1\r\n",
      "chrM:78\t1\r\n",
      "chrM:79\t1\r\n",
      "chrM:80\t1\r\n",
      "chrM:81\t1\r\n",
      "chrM:82\t1\r\n",
      "chrM:83\t1\r\n",
      "chrM:84\t1\r\n",
      "chrM:85\t1\r\n",
      "chrM:86\t1\r\n",
      "chrM:87\t1\r\n",
      "chrM:88\t1\r\n",
      "chrM:14\t1\r\n",
      "chrM:15\t1\r\n",
      "chrM:16\t1\r\n",
      "chrM:17\t1\r\n",
      "chrM:18\t1\r\n",
      "chrM:19\t1\r\n",
      "chrM:20\t1\r\n",
      "chrM:21\t1\r\n",
      "chrM:22\t1\r\n",
      "chrM:23\t1\r\n",
      "chrM:24\t1\r\n",
      "chrM:25\t1\r\n",
      "chrM:26\t1\r\n",
      "chrM:27\t1\r\n",
      "chrM:28\t1\r\n",
      "chrM:29\t1\r\n",
      "chrM:30\t1\r\n",
      "chrM:31\t1\r\n",
      "chrM:32\t1\r\n",
      "chrM:33\t1\r\n",
      "chrM:34\t1\r\n",
      "chrM:35\t1\r\n",
      "chrM:36\t1\r\n",
      "chrM:37\t1\r\n",
      "chrM:38\t1\r\n",
      "chrM:39\t1\r\n",
      "chrM:40\t1\r\n",
      "chrM:41\t1\r\n",
      "chrM:42\t1\r\n",
      "chrM:43\t1\r\n",
      "chrM:44\t1\r\n",
      "chrM:45\t1\r\n",
      "chrM:46\t1\r\n",
      "chrM:47\t1\r\n",
      "chrM:48\t1\r\n",
      "chrM:49\t1\r\n",
      "chrM:50\t1\r\n",
      "chrM:51\t1\r\n",
      "chrM:52\t1\r\n",
      "chrM:53\t1\r\n",
      "chrM:54\t1\r\n",
      "chrM:55\t1\r\n",
      "chrM:56\t1\r\n",
      "chrM:57\t1\r\n",
      "chrM:58\t1\r\n",
      "chrM:59\t1\r\n",
      "chrM:60\t1\r\n",
      "chrM:61\t1\r\n",
      "chrM:62\t1\r\n",
      "chrM:63\t1\r\n",
      "chrM:64\t1\r\n",
      "chrM:65\t1\r\n",
      "chrM:66\t1\r\n",
      "chrM:67\t1\r\n",
      "chrM:68\t1\r\n",
      "chrM:69\t1\r\n",
      "chrM:70\t1\r\n",
      "chrM:71\t1\r\n",
      "chrM:72\t1\r\n",
      "chrM:73\t1\r\n",
      "chrM:74\t1\r\n",
      "chrM:75\t1\r\n",
      "chrM:76\t1\r\n",
      "chrM:77\t1\r\n",
      "chrM:78\t1\r\n",
      "chrM:79\t1\r\n",
      "chrM:80\t1\r\n",
      "chrM:81\t1\r\n",
      "chrM:82\t1\r\n",
      "chrM:83\t1\r\n",
      "chrM:84\t1\r\n",
      "chrM:85\t1\r\n",
      "chrM:86\t1\r\n",
      "chrM:87\t1\r\n",
      "chrM:88\t1\r\n",
      "chrM:19\t1\r\n",
      "chrM:20\t1\r\n",
      "chrM:21\t1\r\n",
      "chrM:22\t1\r\n",
      "chrM:23\t1\r\n",
      "chrM:24\t1\r\n",
      "chrM:25\t1\r\n",
      "chrM:26\t1\r\n",
      "chrM:27\t1\r\n",
      "chrM:28\t1\r\n",
      "chrM:29\t1\r\n",
      "chrM:30\t1\r\n",
      "chrM:31\t1\r\n",
      "chrM:32\t1\r\n",
      "chrM:33\t1\r\n",
      "chrM:34\t1\r\n",
      "chrM:35\t1\r\n",
      "chrM:36\t1\r\n",
      "chrM:37\t1\r\n",
      "chrM:38\t1\r\n",
      "chrM:39\t1\r\n",
      "chrM:40\t1\r\n",
      "chrM:41\t1\r\n",
      "chrM:42\t1\r\n",
      "chrM:43\t1\r\n",
      "chrM:44\t1\r\n",
      "chrM:45\t1\r\n",
      "chrM:46\t1\r\n",
      "chrM:47\t1\r\n",
      "chrM:48\t1\r\n",
      "chrM:49\t1\r\n",
      "chrM:50\t1\r\n",
      "chrM:51\t1\r\n",
      "chrM:52\t1\r\n",
      "chrM:53\t1\r\n",
      "chrM:54\t1\r\n",
      "chrM:55\t1\r\n",
      "chrM:56\t1\r\n",
      "chrM:57\t1\r\n",
      "chrM:58\t1\r\n",
      "chrM:59\t1\r\n",
      "chrM:60\t1\r\n",
      "chrM:61\t1\r\n",
      "chrM:62\t1\r\n",
      "chrM:63\t1\r\n",
      "chrM:64\t1\r\n",
      "chrM:65\t1\r\n",
      "chrM:66\t1\r\n",
      "chrM:67\t1\r\n",
      "chrM:68\t1\r\n",
      "chrM:69\t1\r\n",
      "chrM:70\t1\r\n",
      "chrM:71\t1\r\n",
      "chrM:72\t1\r\n",
      "chrM:73\t1\r\n",
      "chrM:74\t1\r\n",
      "chrM:75\t1\r\n",
      "chrM:76\t1\r\n",
      "chrM:77\t1\r\n",
      "chrM:78\t1\r\n",
      "chrM:79\t1\r\n",
      "chrM:80\t1\r\n",
      "chrM:81\t1\r\n",
      "chrM:82\t1\r\n",
      "chrM:83\t1\r\n",
      "chrM:84\t1\r\n",
      "chrM:85\t1\r\n",
      "chrM:86\t1\r\n",
      "chrM:87\t1\r\n",
      "chrM:88\t1\r\n",
      "chrM:89\t1\r\n",
      "chrM:90\t1\r\n",
      "chrM:91\t1\r\n",
      "chrM:92\t1\r\n",
      "chrM:93\t1\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 100 $myfile | python mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle step \n",
    "\n",
    "A lot happens, transparent to the developer\n",
    "Mappers’s output is transformed and distributed to the reducers\n",
    "All key/value pairs are sorted before sent to reducer function\n",
    "Pairs sharing the same key are sent to the same reducer\n",
    "If you encounter a key that is different from the last key you processed, you know that previous key will never appear again\n",
    "If your keys are all the same\n",
    "only use one reducer and gain no parallelization\n",
    "come up with a more unique key if this happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "last_value = \"\"\n",
    "value_count = 1\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    value, count = line.split(TAB)\n",
    "    count = int(count)\n",
    "\n",
    "    # if this is the first iteration\n",
    "    if not last_value:\n",
    "        last_value = value\n",
    "\n",
    "    # if they're the same, log it\n",
    "    if value == last_value:\n",
    "        value_count += count\n",
    "    else:\n",
    "        # state change\n",
    "        try: \n",
    "            print(TAB.join([last_value, str(value_count)]))\n",
    "        except:\n",
    "            pass\n",
    "        last_value = value\n",
    "        value_count = 1\n",
    "\n",
    "# LAST ONE after all records have been received\n",
    "print(TAB.join([last_value, str(value_count)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1:10000\t3\n",
      "chr1:10001\t2\n",
      "chr1:10002\t2\n",
      "chr1:10003\t2\n",
      "chr1:10004\t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m5.921s\n",
      "user\t0m5.340s\n",
      "sys\t0m0.150s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# needs ~ 10 seconds for running\n",
    "time head -n 10000 worker/ngs.sam | python mapper.py | sort | python reducer.py | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t3m30.950s\n",
      "user\t3m8.630s\n",
      "sys\t0m10.660s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "time cat worker/ngs.sam | python mapper.py | sort | python reducer.py > worker/out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Switching to real Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A working python code tested on pipes should work with Hadoop Streaming\n",
    "\n",
    "To make this work we need to handle copy of input and output file \n",
    "inside the Hadoop FS\n",
    "Also the job tracker logs will be found inside HDFS\n",
    "We are going to build a bash script to make our workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HDFS commands to interact with Hadoop file system\n",
    "\n",
    "Create dir \n",
    "hadoop fs -mkdir\n",
    "Copy file\n",
    "hadoop fs -put\n",
    "Check if file is there\n",
    "hadoop fs -ls\n",
    "Remove recursively data\n",
    "hadoop fs -rmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hadoop Streaming needs “binaries” to execute\n",
    "\n",
    "You need to specify interpreter inside the script\n",
    "#!/usr/bin/env python\n",
    "Make the script executable\n",
    "chmod +x hs*.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final launch via bash command for using Hadoop streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts on Hadoop Streaming \n",
    "\n",
    "Provides options to write MapReduce jobs in other languages\n",
    "One of the best examples of flexibility available to MapReduce\n",
    "Fast\n",
    "Simple\n",
    "Close to the original standard Java API power\n",
    "\n",
    "Even executables can be used to work as a MapReduce job (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where it really works\n",
    "\n",
    "When the developer do not have knowhow of Java \n",
    "Write Mapper/Reducer in any scripting language \n",
    "Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons\n",
    "\n",
    "Force scripts in a Java VM\n",
    "Although free overhead\n",
    "The program/executable should be able to take input from STDIN and produce output at STDOUT\n",
    "Restrictions on the input/output formats\n",
    "Does not take care of input and output file and directory preparation\n",
    "User have to implement hdfs commands “hand-made”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where it falls short\n",
    "\n",
    "No pythonic way to work the MapReduce code\n",
    "Because it was not written specifically for python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap \n",
    "\n",
    "Hadoop streaming handles Hadoop in almost a classic manner\n",
    "Wrap any executable (and script)\n",
    "Wrap python scripts\n",
    "Runnable on a cluster\n",
    "using a non-interactive, all-encapsulated job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Docker lightweight virtualization\n",
    "* Two environments \n",
    "* Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
