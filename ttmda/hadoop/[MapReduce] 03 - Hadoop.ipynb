{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop\n",
    "(and Hadoop streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add logo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map/reduce is a completely different paradigm \n",
    "Solving a certain subset of parallelizable problems \n",
    "around the bottleneck of ingesting input data from disk\n",
    "Traditional parallelism brings the data to the computing machine\n",
    "Map/reduce does the opposite, it brings the compute to the data\n",
    "Input data is not stored on a separate storage system\n",
    "Data exists in little pieces and is permanently stored on each computing node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFS is fundamental to Hadoop \n",
    "it provides the data chunking \n",
    "distribution across compute elements \n",
    "necessary for map/reduce applications to be efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word count\n",
    "The '`Hello World`' for MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the simplest of full Hadoop jobs you can run\n",
    "\n",
    "**IMAGE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAP step will take the raw text and convert it to key/value pairs\n",
    "Each key is a word\n",
    "All keys (words) will have a value of 1\n",
    "The REDUCE step will combine all duplicate keys \n",
    "By adding up their values (sum)\n",
    "Every key (word) has a value of 1 (Map)\n",
    "Output is reduced to a list of unique keys\n",
    "Each key’s value corresponding to key's (word's) count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map function : processes data and generates a set of  intermediate key/value pairs.\n",
    "Reduce function : merges all intermediate values  associated with the same intermediate key.\n",
    "\n",
    "**IMAGE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A WordCount example \n",
    "*(with Java)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider doing a word count of the following file \n",
    "using  MapReduce:\n",
    "\n",
    "```\n",
    "$ cat file.txt\n",
    "\n",
    "Hello World Bye World\n",
    "Hello Hadoop Goodbye Hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map function reads in words \n",
    "one at a time \n",
    "outputs (“word”, 1) for each parsed input word\n",
    "\n",
    "Map output is:\n",
    "```\n",
    "(Hello, 1)\n",
    "(World, 1)\n",
    "(Bye, 1)\n",
    "(World, 1)\n",
    "(Hello, 1)\n",
    "(Hadoop, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shuffle phase between map and reduce \n",
    "creates a  list of values associated with each key\n",
    "\n",
    "Shuffle output / Reduce input is:\n",
    "\n",
    "```\n",
    "(Bye, (1))\n",
    "(Goodbye, (1))\n",
    "(Hadoop, (1, 1))\n",
    "(Hello, (1, 1))\n",
    "(World, (1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce function sums the numbers in the list \n",
    "for each  key and outputs (word, count) pairs\n",
    "\n",
    "Reduce output is: \n",
    "(also the output of the  MapReduce job)\n",
    "\n",
    "```\n",
    "(Bye, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 2)\n",
    "(Hello, 2)\n",
    "(World, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Java\n",
    "package org.myorg;\n",
    "import java.io.IOException;\n",
    "import java.util.*;\n",
    "import org.apache.hadoop.*\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "  public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      String line = value.toString();\n",
    "      StringTokenizer tokenizer = new StringTokenizer(line);\n",
    "      while (tokenizer.hasMoreTokens()) {\n",
    "        word.set(tokenizer.nextToken());\n",
    "        output.collect(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "    \n",
    "  public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      int sum = 0;\n",
    "      while (values.hasNext()) {\n",
    "        sum += values.next().get();\n",
    "      }\n",
    "      output.collect(key, new IntWritable(sum));\n",
    "    }\n",
    "  }\n",
    "    \n",
    "  \n",
    "  public static void main(String[] args) throws Exception {\n",
    "    JobConf conf = new JobConf(WordCount.class);\n",
    "    conf.setJobName(\"wordcount\");\n",
    "\n",
    "    conf.setOutputKeyClass(Text.class);\n",
    "    conf.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    conf.setMapperClass(Map.class);\n",
    "    conf.setCombinerClass(Reduce.class);\n",
    "    conf.setReducerClass(Reduce.class);\n",
    "\n",
    "    conf.setInputFormat(TextInputFormat.class);\n",
    "    conf.setOutputFormat(TextOutputFormat.class);\n",
    "\n",
    "    FileInputFormat.setInputPaths(conf, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(conf, new Path(args[1]));\n",
    "\n",
    "    JobClient.runJob(conf);\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ hadoop jar hadoop-examples.jar wordcount dft dft-output\n",
    "\n",
    "10/03/16 11:40:51 INFO mapred.FileInputFormat: Total input paths to process : 1 10/03/16 11:40:51 INFO mapred.JobClient: Running job: job_201003161102_0002 10/03/16 11:40:52 INFO mapred.JobClient: map 0% reduce 0%\n",
    "10/03/16 11:40:55 INFO mapred.JobClient: map 9% reduce 0% 10/03/16 11:40:56 INFO mapred.JobClient: map 27% reduce 0%\n",
    "10/03/16 11:40:58 INFO mapred.JobClient: map 45% reduce 0% 10/03/16 11:40:59 INFO mapred.JobClient: map 81% reduce 0% \n",
    "\n",
    "10/03/16 11:41:01 INFO mapred.JobClient: map 100% reduce 0% 10/03/16 11:41:09 INFO mapred.JobClient: Job complete: job_201003161102_0002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ hadoop dfs -cat dft-output/part-00000 | less \n",
    "\n",
    "“Information\" 1 \n",
    "\"J\" 1 \n",
    "“Plain\" 2 \n",
    "“Project\" 5 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGE**: cutting sandwiches with hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Write a Java class to count letters inside a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*...just kidding!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "head -2000 data/ngs/input.sam | tail -n 10 \\\n",
    "| awk '{ print $3\":\"$4 }' | sort | uniq -c\n",
    "\n",
    "      3 chr1:142803456\n",
    "      3 chr1:142803458\n",
    "      1 chr1:142803465\n",
    "      1 chr1:142803470\n",
    "      2 chr1:142803471\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: myfile=/data/worker/books/pg1232.txt\n"
     ]
    }
   ],
   "source": [
    "myfile = '/data/worker/books/pg1232.txt'\n",
    "%env myfile $myfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 :\n",
      "      1 and:turned\n",
      "      1 are:found\n",
      "      1 is:gained\n",
      "      1 mercenaries,:when\n",
      "      1 needed:to\n",
      "      1 than:to\n",
      "      1 their:head,\n",
      "      1 valour.:The\n",
      "      1 you.:In\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `head -2000 data/ngs/input.sam | tail -n 10`\n",
    "\n",
    "1. `awk '{ print $3\":\"$4 }’`\n",
    "\n",
    "1. `sort`\n",
    "\n",
    "1. `uniq -c`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INPUT STREAM**\n",
    "`head -2000 data/ngs/input.sam | tail -n 10`\n",
    "\n",
    "**MAPPER**\n",
    "`awk '{ print $3\":\"$4 }'`\n",
    "\n",
    "**SHUFFLE**\n",
    "`sort`\n",
    "\n",
    "**REDUCER**\n",
    "`uniq -c`\n",
    "\n",
    "**OUTPUT STREAM**\n",
    "`<STDOUT>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# head -2000 data/ngs/input.sam | tail -n 10\n",
    "HSCAN:421:C47DAACXX:3:1206:5660:99605\t99\tchr1\t142803456\t10\t75M\t=\t142803517\t136\tCTGTGCATTCTTATGATTTTAATATTCTGTACATTTATTATTGATTTAAAATGCATTTTACCTTTTTCTTTAATA\t@@CDDDD?DBFHFHIHIJJJJGHEIFHIIIHGDFEGEIAHAHGIIGIIIC?CBFCGHFCEG?@DGIIIGHIGGHC\tX0:i:3\tX1:i:1\tXA:Z:chr21,+9746045,75M,0;chr1,+143355186,75M,0;chr1,-143233123,75M,1;\tMD:Z:75\tRG:Z:1\tXG:i:0\tAM:i:0\tNM:i:0\tSM:i:0\tXM:i:0\tXO:i:0\tMQ:i:18\tXT:A:R\n",
    "\n",
    "[…]\n",
    "\n",
    "# head -2000 data/ngs/input.sam | tail -n 10 | awk '{ print $3\":\"$4 }'\n",
    "chr1:142803471\n",
    "chr1:142803456\n",
    "chr1:142803465\n",
    "chr1:142803458\n",
    "chr1:142803456\n",
    "chr1:142803471\n",
    "chr1:142803458\n",
    "chr1:142803456\n",
    "chr1:142803470\n",
    "chr1:142803458\n",
    "\n",
    "# head -2000 data/ngs/input.sam | tail -n 10 | awk '{ print $3\":\"$4 }' | sort\n",
    "chr1:142803456\n",
    "chr1:142803456\n",
    "chr1:142803456\n",
    "chr1:142803458\n",
    "chr1:142803458\n",
    "chr1:142803458\n",
    "chr1:142803465\n",
    "chr1:142803470\n",
    "chr1:142803471\n",
    "chr1:142803471\n",
    "\n",
    "# head -2000 data/ngs/input.sam | tail -n 10 | awk '{ print $3\":\"$4 }' | sort | uniq -c\n",
    "      3 chr1:142803456\n",
    "      3 chr1:142803458\n",
    "      1 chr1:142803465\n",
    "      1 chr1:142803470\n",
    "      2 chr1:142803471\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations with bash pipes as simulation of MapReduce\n",
    "\n",
    "* Serial steps\n",
    "* No file distribution\n",
    "* Single node\n",
    "* Single mapper\n",
    "* Single reducer\n",
    "* Can we add a Combiner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop streaming\n",
    "### Concepts and mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop streaming is a utility \n",
    "comes with the Hadoop distribution\n",
    "Allows to create and run Map/Reduce jobs \n",
    "with any executable or script as the mapper and/or the reducer\n",
    "Protocol steps:\n",
    "Create a Map/Reduce job\n",
    "Submit the job to an appropriate cluster\n",
    "Monitor the progress of the job until it completes\n",
    "Links to Hadoop HDFS job directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why?\n",
    "One of the unappetizing aspects of Hadoop to users of traditional HPC is that it is written in Java. \n",
    "Java is not originally designed to be a high-performance language\n",
    "Learning it is not easy for domain scientists\n",
    " Hadoop allows you to write map/reduce code in any language you want using the Hadoop Streaming interface\n",
    "It means turning an existing Python or Perl script into a Hadoop job\n",
    "Does not require learning any Java at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce with binaries/executables\n",
    "Executable is specified for mappers and reducers\n",
    "each mapper task will launch the executable as a separate process \n",
    "converts inputs into lines and feed to the stdin of the process\n",
    "the mapper collects the line oriented outputs from the stdout of the process \n",
    "converts each line into a key/value pair\n",
    "By default, the prefix of a line up to the first tab character is the key and the rest of the line (excluding the tab character) will be the value\n",
    "e.g. ”this is the key\\tvalue is the rest\\n”\n",
    "If there is no tab character in the line, then entire line is considered as key and the value is null (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A streaming command line example:\n",
    "```\n",
    "$ hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -input myInputDirs \\\n",
    "    -output myOutputDir \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer /bin/wc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A streaming command line example **for python**:\n",
    "```\n",
    "$ hadoop jar hadoop-streaming-1.2.1.jar \\\n",
    "    -input input_dir/ \\\n",
    "    -output output_dir/ \\\n",
    "    -mapper mapper.py \\\n",
    "    -file mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -file reducer.py \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting the Hadoop Streaming job:\n",
    "\n",
    "* Make sure your scripts have no errors\n",
    "* Do mapper and reducer scripts actually work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a matter of running them through pipes on a **little bit** of sample data,\n",
    "\n",
    "like `cat` or `head` linux bash commands, with pipes, as seen before.\n",
    "\n",
    "```\n",
    "# Simulating hadoop streaming with bash pipes\n",
    "$ cat $file | python mapper.py | sort | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First approach: split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    pieces = line.split('\\t')\n",
    "\n",
    "print(pieces) \n",
    "#prints [“piece1”,”piece2”…]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"mapper.py\", line 4, in <module>\r\n",
      "    for line in sys.stdin:\r\n",
      "  File \"/opt/conda/lib/python3.4/encodings/ascii.py\", line 26, in decode\r\n",
      "    return codecs.ascii_decode(input, self.errors)[0]\r\n",
      "UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 10 $myfile | python mapper.py \n",
    "# | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "\n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "    print mychr,mystart.__str__()\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#cat data/ngs/input.sam \\ \n",
    "| python ngs/hs/hsmapper.py\n",
    "\n",
    "chrM 14\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "\n",
    "    mystop = mystart + len(myseq)\n",
    "\n",
    "    # Each element with coverage\n",
    "    for i in range(mystart,mystop):\n",
    "        results = [mychr+SEP+i.__str__(), \"1\"]\n",
    "        print(TAB.join(results))\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "root@5a57dab12438:/course-exercises\n",
    "\n",
    "#cat data/ngs/input.sam \\ \n",
    "| python ngs/hs/hsmapper.py\n",
    "\n",
    "chrM 14 1\n",
    "\n",
    "[…]\n",
    "\n",
    "chrM:79\t1\n",
    "chrM:80\t1\n",
    "chrM:81\t1\n",
    "chrM:82\t1\n",
    "chrM:83\t1\n",
    "chrM:84\t1\n",
    "chrM:85\t1\n",
    "chrM:86\t1\n",
    "chrM:87\t1\n",
    "chrM:88\t1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle step \n",
    "\n",
    "A lot happens, transparent to the developer\n",
    "Mappers’s output is transformed and distributed to the reducers\n",
    "All key/value pairs are sorted before sent to reducer function\n",
    "Pairs sharing the same key are sent to the same reducer\n",
    "If you encounter a key that is different from the last key you processed, you know that previous key will never appear again\n",
    "If your keys are all the same\n",
    "only use one reducer and gain no parallelization\n",
    "come up with a more unique key if this happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    value, count = line.split(TAB)\n",
    "    count = int(count)\n",
    "\n",
    "    # if this is the first iteration\n",
    "    if not last_value:\n",
    "        last_value = value\n",
    "\n",
    "    # if they're the same, log it\n",
    "    if value == last_value:\n",
    "        value_count += count\n",
    "    else:\n",
    "        # state change\n",
    "        result = [last_value, value_count]\n",
    "        print TAB.join(str(v) for v in result)\n",
    "        last_value = value\n",
    "        value_count = 1\n",
    "\n",
    "# LAST ONE after all records have been received\n",
    "print TAB.join(str(v) for v in [last_value, value_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "root@5a57dab12438:/course-exercises\n",
    "\n",
    "#cat data/ngs/input.sam \\ \n",
    "| python ngs/hs/hsmapper.py \\\n",
    "| sort \\\n",
    "| python ngs/hs/hsreducer.py\n",
    "\n",
    "chr10:103270024\t1\n",
    "chr10:103270025\t1\n",
    "\n",
    "[…]\n",
    "\n",
    "chr21:48110960\t2\n",
    "chr21:48110961\t3\n",
    "chr21:48110962\t3\n",
    "chr21:48110963\t6\n",
    "chr21:48110964\t6\n",
    "chr21:48110965\t6\n",
    "chr21:48110966\t6\n",
    "chr21:48110967\t8\n",
    "chr21:48110968\t8\n",
    "chr21:48110969\t11 \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching to real Hadoop\n",
    "\n",
    "A working python code tested on pipes should work with Hadoop Streaming\n",
    "\n",
    "To make this work we need to handle copy of input and output file \n",
    "inside the Hadoop FS\n",
    "Also the job tracker logs will be found inside HDFS\n",
    "We are going to build a bash script to make our workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final launch via bash command for using Hadoop streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts on Hadoop Streaming \n",
    "\n",
    "Provides options to write MapReduce jobs in other languages\n",
    "One of the best examples of flexibility available to MapReduce\n",
    "Fast\n",
    "Simple\n",
    "Close to the original standard Java API power\n",
    "\n",
    "Even executables can be used to work as a MapReduce job (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where it really works\n",
    "\n",
    "When the developer do not have knowhow of Java \n",
    "Write Mapper/Reducer in any scripting language \n",
    "Faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons\n",
    "\n",
    "Force scripts in a Java VM\n",
    "Although free overhead\n",
    "The program/executable should be able to take input from STDIN and produce output at STDOUT\n",
    "Restrictions on the input/output formats\n",
    "Does not take care of input and output file and directory preparation\n",
    "User have to implement hdfs commands “hand-made”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where it falls short\n",
    "\n",
    "No pythonic way to work the MapReduce code\n",
    "Because it was not written specifically for python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap \n",
    "\n",
    "Hadoop streaming handles Hadoop in almost a classic manner\n",
    "Wrap any executable (and script)\n",
    "Wrap python scripts\n",
    "Runnable on a cluster\n",
    "using a non-interactive, all-encapsulated job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Docker lightweight virtualization\n",
    "* Two environments \n",
    "* Schema"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
