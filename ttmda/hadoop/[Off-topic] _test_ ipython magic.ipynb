{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simpler MapReduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker/books/pg1232.txt\r\n"
     ]
    }
   ],
   "source": [
    "file = \"worker/books/pg1232.txt\"\n",
    "!ls {file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Â A MrJob dedicated extension\n",
    "\n",
    "Creating an extension for helping users with mapreduce in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the extension\n",
    "# note: reload_ext does not get error if you try two times\n",
    "%reload_ext mrjobmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How it works:\n",
    "\n",
    "* Cell extension\n",
    "\n",
    "```\n",
    "%%mapreduce LOCAL_INPUT [inline,hadoop]\n",
    "\n",
    "MAPPER function\n",
    "[COMBINER function]\n",
    "REDUCER function\n",
    "```\n",
    "\n",
    "* Line extension\n",
    "\n",
    "```\n",
    "%mapreduce LOCAL_INPUT MR_FILE [inline,hadoop]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%mapreduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cell execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Executing inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file is worker/books/pg1232.txt\n",
      "Saving jobs/script_000009063.py\n",
      "Executing python3 jobs/script_000009063.py -r inline worker/books/pg1232.txt\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/script_000009063.root.20151009.155741.060877\n",
      "writing to /tmp/script_000009063.root.20151009.155741.060877/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/script_000009063.root.20151009.155741.060877/step-0-mapper-sorted\n",
      "> sort /tmp/script_000009063.root.20151009.155741.060877/step-0-mapper_part-00000\n",
      "writing to /tmp/script_000009063.root.20151009.155741.060877/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /tmp/script_000009063.root.20151009.155741.060877/step-0-reducer_part-00000 -> /tmp/script_000009063.root.20151009.155741.060877/output/part-00000\n",
      "Streaming final output from /tmp/script_000009063.root.20151009.155741.060877/output\n",
      "removing tmp directory /tmp/script_000009063.root.20151009.155741.060877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jobs/script_000009063.py'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%mapreduce $file\n",
    "\n",
    "def mapper(self, _, line):\n",
    "    pass\n",
    "def reducer(self, key, line):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Executing on the existing Hadoop cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file is worker/books/pg1232.txt\n",
      "Saving jobs/script_000093882.py\n",
      "Executing python3 jobs/script_000093882.py -r hadoop worker/books/pg1232.txt\n",
      "Unexpected option hdfs_tmp_dir\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/script_000093882.root.20151009.150448.164233\n",
      "writing wrapper script to /tmp/script_000093882.root.20151009.150448.164233/setup-wrapper.sh\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files into hdfs:///user/root/tmp/mrjob/script_000093882.root.20151009.150448.164233/files/\n",
      "\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "HADOOP: Total input paths to process : 1\n",
      "HADOOP: number of splits:2\n",
      "HADOOP: Submitting tokens for job: job_1444400400051_0001\n",
      "HADOOP: Submitted application application_1444400400051_0001\n",
      "HADOOP: The url to track the job: http://ipyhadoop:8088/proxy/application_1444400400051_0001/\n",
      "HADOOP: Running job: job_1444400400051_0001\n",
      "HADOOP: Job job_1444400400051_0001 running in uber mode : false\n",
      "HADOOP:  map 0% reduce 0%\n",
      "HADOOP:  map 100% reduce 0%\n",
      "HADOOP:  map 100% reduce 100%\n",
      "HADOOP: Job job_1444400400051_0001 completed successfully\n",
      "HADOOP: Counters: 49\n",
      "HADOOP: \tFile System Counters\n",
      "HADOOP: \t\tFILE: Number of bytes read=6\n",
      "HADOOP: \t\tFILE: Number of bytes written=333908\n",
      "HADOOP: \t\tFILE: Number of read operations=0\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\n",
      "HADOOP: \t\tFILE: Number of write operations=0\n",
      "HADOOP: \t\tHDFS: Number of bytes read=308890\n",
      "HADOOP: \t\tHDFS: Number of bytes written=0\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\n",
      "HADOOP: \tJob Counters \n",
      "HADOOP: \t\tLaunched map tasks=2\n",
      "HADOOP: \t\tLaunched reduce tasks=1\n",
      "HADOOP: \t\tData-local map tasks=2\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=29069\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=7003\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=29069\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=7003\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=29069\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=7003\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=29766656\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=7171072\n",
      "HADOOP: \tMap-Reduce Framework\n",
      "HADOOP: \t\tMap input records=5063\n",
      "HADOOP: \t\tMap output records=0\n",
      "HADOOP: \t\tMap output bytes=0\n",
      "HADOOP: \t\tMap output materialized bytes=12\n",
      "HADOOP: \t\tInput split bytes=310\n",
      "HADOOP: \t\tCombine input records=0\n",
      "HADOOP: \t\tCombine output records=0\n",
      "HADOOP: \t\tReduce input groups=0\n",
      "HADOOP: \t\tReduce shuffle bytes=12\n",
      "HADOOP: \t\tReduce input records=0\n",
      "HADOOP: \t\tReduce output records=0\n",
      "HADOOP: \t\tSpilled Records=0\n",
      "HADOOP: \t\tShuffled Maps =2\n",
      "HADOOP: \t\tFailed Shuffles=0\n",
      "HADOOP: \t\tMerged Map outputs=2\n",
      "HADOOP: \t\tGC time elapsed (ms)=383\n",
      "HADOOP: \t\tCPU time spent (ms)=2130\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=484282368\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=2110332928\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=257433600\n",
      "HADOOP: \tShuffle Errors\n",
      "HADOOP: \t\tBAD_ID=0\n",
      "HADOOP: \t\tCONNECTION=0\n",
      "HADOOP: \t\tIO_ERROR=0\n",
      "HADOOP: \t\tWRONG_LENGTH=0\n",
      "HADOOP: \t\tWRONG_MAP=0\n",
      "HADOOP: \t\tWRONG_REDUCE=0\n",
      "HADOOP: \tFile Input Format Counters \n",
      "HADOOP: \t\tBytes Read=308580\n",
      "HADOOP: \tFile Output Format Counters \n",
      "HADOOP: \t\tBytes Written=0\n",
      "HADOOP: Output directory: hdfs:///user/root/tmp/mrjob/script_000093882.root.20151009.150448.164233/output\n",
      "looking for job logs in /usr/local/hadoop/logs/history/\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/script_000093882.root.20151009.150448.164233/output\n",
      "removing tmp directory /tmp/script_000093882.root.20151009.150448.164233\n",
      "deleting hdfs:///user/root/tmp/mrjob/script_000093882.root.20151009.150448.164233 from HDFS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%mapreduce $file hadoop\n",
    "\n",
    "def mapper(self, _, line):\n",
    "    pass\n",
    "def reducer(self, key, line):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Line execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Executing an existing file\n",
    "\n",
    "<small>Note: `$_` will recall latest notebook cell output</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file is worker/books/pg1232.txt\n",
      "Only line\n",
      "Executing python3 jobs/script_000009063.py -r inline worker/books/pg1232.txt\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/script_000009063.root.20151009.155745.218118\n",
      "writing to /tmp/script_000009063.root.20151009.155745.218118/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/script_000009063.root.20151009.155745.218118/step-0-mapper-sorted\n",
      "> sort /tmp/script_000009063.root.20151009.155745.218118/step-0-mapper_part-00000\n",
      "writing to /tmp/script_000009063.root.20151009.155745.218118/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /tmp/script_000009063.root.20151009.155745.218118/step-0-reducer_part-00000 -> /tmp/script_000009063.root.20151009.155745.218118/output/part-00000\n",
      "Streaming final output from /tmp/script_000009063.root.20151009.155745.218118/output\n",
      "removing tmp directory /tmp/script_000009063.root.20151009.155745.218118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jobs/script_000009063.py'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%mapreduce $file $_"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
