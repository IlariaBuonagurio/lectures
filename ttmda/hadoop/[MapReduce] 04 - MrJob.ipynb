{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A more pythonic MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With Hadoop Streaming we switched MapReduce from Java to Python.\n",
    "\n",
    "How could we improve some more? \n",
    "\n",
    "What problems do we deal with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have to directly deal with HDFS\n",
    "* move input files\n",
    "* recover outputs\n",
    "* human make mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not easy debugging\n",
    "\n",
    "* logs via jobtracker\n",
    "* errors are Java stacktrace, often unrelated with the real problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have to write two different files\n",
    "* one for the mapper\n",
    "* one for the reducer\n",
    "* not a **module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is there a Python part of the language that can help representing a MapReduce task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A python class\n",
    "\n",
    "class myclass(object):\n",
    "    \n",
    "    a_property = 42\n",
    "    \n",
    "    def a_method(self):\n",
    "        pass\n",
    "    def another_method(self, var):\n",
    "        print(var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create instance of our class\n",
    "instance = myclass()\n",
    "# Use it\n",
    "instance.a_method()\n",
    "instance.another_method(\"test\")\n",
    "instance.a_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MapReduce(object):\n",
    "    \"\"\" A MapReduce class prototype \"\"\"\n",
    "    \n",
    "    def mapper(self, line):\n",
    "        pass\n",
    "    def reducer(self, sorted_line):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MRJob \n",
    "A more pythonic MapReduce library from Yelp\n",
    "\n",
    "<img src='https://avatars1.githubusercontent.com/u/49071?v=3&s=400' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> “Easiest route to Python programs that run on Hadoop”\n",
    "\n",
    "Install with: \n",
    "```bash\n",
    "pip install mrjob\n",
    "```\n",
    "\n",
    "**Running modes**\n",
    "* Test your code locally without installing Hadoop \n",
    "* or run it on a cluster of your choice!\n",
    "    - Integrates with Amazon **Elastic MapReduce** (EMR)\n",
    "    - same code with local, Hadoop, EMR\n",
    "    - easy to run your job in the cloud as on your laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does MrJob work?\n",
    "\n",
    "Python module built on top of Hadoop Streaming\n",
    "jar opens a subprocess to your code\n",
    "sends it input via stdin\n",
    "gathers results via stdout.\n",
    "Wrap HDFS pre and post processing if hadoop exists\n",
    "a consistent interface across every environment it supports\n",
    "automatically serializes/deserializes data flow out of each task \n",
    "JSON: json.loads() and json.dumps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting hands dirty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A job is defined by a class extended from MRJob package\n",
    "Contains methods that define the steps of a Hadoop job\n",
    "A “step” consists of a mapper, a combiner, and a reducer. \n",
    "All of those  are optional, though you must have at least on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class myjob(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        pass\n",
    "    def combiner(self, key, values):\n",
    "        pass\n",
    "    def reducer(self, key, values):\n",
    "        pass\n",
    "    def steps(self):\n",
    "        return [ MRStep(mapper=self.mapper, … ), … ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper\n",
    "\n",
    "The mapper() method takes a key and a value as args\n",
    "E.g. key is ignored and a single line of text input is the value\n",
    "Yields as many key-value pairs as it likes\n",
    "Warning: yield != return\n",
    "yield return a generator, the one you usually use with \n",
    "print i; for i in generator\n",
    "Example\n",
    "```python\n",
    "def mygen():\n",
    "for i in range(1,10):\n",
    "# THIS IS WHAT HAPPENS INSIDE MAPPER\n",
    "yield i, “value” \n",
    "\n",
    "for key, value in mygen():\n",
    "print key, value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer\n",
    "\n",
    "The reduce() method takes a key and an iterator of values\n",
    "Also yields as many key-value pairs as it likes\n",
    "E.g. it sums the values for each key\n",
    "Represent the  numbers of characters, words, and lines in the initial input\n",
    "\n",
    "Example\n",
    "```python\n",
    "def mygen():\n",
    "for i in range(1,10):\n",
    "yield i, “value” \n",
    "\n",
    "for key, value in mygen():\n",
    "# THIS IS WHAT HAPPENS INSIDE A REDUCER\n",
    "print key, value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's write our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: mydir=mymrjob\n",
      "env: myinput=/data/worker/books/twolines.txt\n",
      "env: myscript=mymrjob/wordcount.py\n",
      "env: myoutput=mymrjob/out.txt\n",
      "env: mylog=mymrjob/out.log\n"
     ]
    }
   ],
   "source": [
    "# Little configuration\n",
    "\n",
    "mydir = \"mymrjob\"\n",
    "%env mydir = $mydir\n",
    "myinput = \"/data/worker/books/twolines.txt\"\n",
    "%env myinput $myinput\n",
    "myscript = mydir + \"/wordcount.py\"\n",
    "%env myscript $myscript\n",
    "\n",
    "%system mkdir -p $mydir\n",
    "%env myoutput $mydir/out.txt\n",
    "%env mylog $mydir/out.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Create the job file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymrjob/wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $myscript\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "class MRWordCount(MRJob):\n",
    "    \"\"\" Wordcount with MapReduce in a pythonic way\"\"\"\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split(' '):\n",
    "             yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O\n",
    "\n",
    "You can pass input via stdin but be aware that mrjob will just dump it to a file first:\n",
    "```bash\n",
    "$ python my_job.py < input.txt\n",
    "```\n",
    "\n",
    "You can pass multiple input files, mixed with stdin (using the – character)\n",
    "```bash\n",
    "$ python my_job.py input1.txt input2.txt - < input3.txt\n",
    "```\n",
    "\n",
    "By default, output will be written to stdout.\n",
    "```bash\n",
    "$ python my_job.py input.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute MrJob\n",
    "! python $myscript $myinput 1> $myoutput 2> $mylog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note again: if this comand takes minutes, you may go to see what is happening in your Hadoop JobTracker\n",
    "http://localhost:8088/cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bye\"\t1\r\n",
      "\"goodbye\"\t1\r\n",
      "\"hadoop\"\t2\r\n",
      "\"hello\"\t2\r\n",
      "\"world\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "%cat $myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected option hdfs_tmp_dir\r\n",
      "no configs found; falling back on auto-configuration\r\n",
      "no configs found; falling back on auto-configuration\r\n",
      "creating tmp directory /tmp/wordcount.root.20151013.105600.184640\r\n",
      "writing wrapper script to /tmp/wordcount.root.20151013.105600.184640/setup-wrapper.sh\r\n",
      "Using Hadoop version 2.6.0\r\n",
      "Copying local files into hdfs:///user/root/tmp/mrjob/wordcount.root.20151013.105600.184640/files/\r\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar7755496883310791297/] [] /tmp/streamjob2462321277901270862.jar tmpDir=null\r",
      "\r\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\r",
      "\r\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\r",
      "\r\n",
      "HADOOP: Total input paths to process : 1\r",
      "\r\n",
      "HADOOP: number of splits:2\r",
      "\r\n",
      "HADOOP: Submitting tokens for job: job_1444732540385_0001\r",
      "\r\n",
      "HADOOP: Submitted application application_1444732540385_0001\r",
      "\r\n",
      "HADOOP: The url to track the job: http://ipyhadoop:8088/proxy/application_1444732540385_0001/\r",
      "\r\n",
      "HADOOP: Running job: job_1444732540385_0001\r",
      "\r\n",
      "HADOOP: Job job_1444732540385_0001 running in uber mode : false\r",
      "\r\n",
      "HADOOP:  map 0% reduce 0%\r",
      "\r\n",
      "HADOOP:  map 100% reduce 0%\r",
      "\r\n",
      "HADOOP:  map 100% reduce 100%\r",
      "\r\n",
      "HADOOP: Job job_1444732540385_0001 completed successfully\r",
      "\r\n",
      "HADOOP: Counters: 49\r",
      "\r\n",
      "HADOOP: \tFile System Counters\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of bytes read=104\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of bytes written=333792\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of read operations=0\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of write operations=0\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of bytes read=374\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of bytes written=51\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\r",
      "\r\n",
      "HADOOP: \tJob Counters \r",
      "\r\n",
      "HADOOP: \t\tLaunched map tasks=2\r",
      "\r\n",
      "HADOOP: \t\tLaunched reduce tasks=1\r",
      "\r\n",
      "HADOOP: \t\tData-local map tasks=2\r",
      "\r\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=11040\r",
      "\r\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=3901\r",
      "\r\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=11040\r",
      "\r\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=3901\r",
      "\r\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=11040\r",
      "\r\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=3901\r",
      "\r\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=11304960\r",
      "\r\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=3994624\r",
      "\r\n",
      "HADOOP: \tMap-Reduce Framework\r",
      "\r\n",
      "HADOOP: \t\tMap input records=2\r",
      "\r\n",
      "HADOOP: \t\tMap output records=8\r",
      "\r\n",
      "HADOOP: \t\tMap output bytes=82\r",
      "\r\n",
      "HADOOP: \t\tMap output materialized bytes=110\r",
      "\r\n",
      "HADOOP: \t\tInput split bytes=300\r",
      "\r\n",
      "HADOOP: \t\tCombine input records=0\r",
      "\r\n",
      "HADOOP: \t\tCombine output records=0\r",
      "\r\n",
      "HADOOP: \t\tReduce input groups=5\r",
      "\r\n",
      "HADOOP: \t\tReduce shuffle bytes=110\r",
      "\r\n",
      "HADOOP: \t\tReduce input records=8\r",
      "\r\n",
      "HADOOP: \t\tReduce output records=5\r",
      "\r\n",
      "HADOOP: \t\tSpilled Records=16\r",
      "\r\n",
      "HADOOP: \t\tShuffled Maps =2\r",
      "\r\n",
      "HADOOP: \t\tFailed Shuffles=0\r",
      "\r\n",
      "HADOOP: \t\tMerged Map outputs=2\r",
      "\r\n",
      "HADOOP: \t\tGC time elapsed (ms)=240\r",
      "\r\n",
      "HADOOP: \t\tCPU time spent (ms)=2260\r",
      "\r\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=526491648\r",
      "\r\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=2127994880\r",
      "\r\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=280240128\r",
      "\r\n",
      "HADOOP: \tShuffle Errors\r",
      "\r\n",
      "HADOOP: \t\tBAD_ID=0\r",
      "\r\n",
      "HADOOP: \t\tCONNECTION=0\r",
      "\r\n",
      "HADOOP: \t\tIO_ERROR=0\r",
      "\r\n",
      "HADOOP: \t\tWRONG_LENGTH=0\r",
      "\r\n",
      "HADOOP: \t\tWRONG_MAP=0\r",
      "\r\n",
      "HADOOP: \t\tWRONG_REDUCE=0\r",
      "\r\n",
      "HADOOP: \tFile Input Format Counters \r",
      "\r\n",
      "HADOOP: \t\tBytes Read=74\r",
      "\r\n",
      "HADOOP: \tFile Output Format Counters \r",
      "\r\n",
      "HADOOP: \t\tBytes Written=51\r",
      "\r\n",
      "HADOOP: Output directory: hdfs:///user/root/tmp/mrjob/wordcount.root.20151013.105600.184640/output\r",
      "\r\n",
      "looking for job logs in /usr/local/hadoop/logs/history/\r\n",
      "Counters from step 1:\r\n",
      "  (no counters found)\r\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/wordcount.root.20151013.105600.184640/output\r\n",
      "removing tmp directory /tmp/wordcount.root.20151013.105600.184640\r\n",
      "deleting hdfs:///user/root/tmp/mrjob/wordcount.root.20151013.105600.184640 from HDFS\r\n"
     ]
    }
   ],
   "source": [
    "%cat $mylog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's an empty **template** to work with in copy/paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile SPECIFY_A_FILENAME.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" MapReduce easily with Python \"\"\"\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class job(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        pass\n",
    "    def reducer(self, key, line):\n",
    "        pass\n",
    "    def steps(self):\n",
    "        return [ MRStep(mapper=self.mapper, reducer=self.reducer)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Convert your exercise for vowels inside the divine comedy into MrJob class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By default, mrjob will run your job on your local (normal) environment) in a single Python process \n",
    "\n",
    "You change the way the job is run with the `-r/--runner` option:\n",
    "\n",
    "```\n",
    "-r inline, -r local, -r hadoop, or -r emr\n",
    "```\n",
    "\n",
    "Use also `--verbose` option to show all the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we just need to add `-r hadoop`.\n",
    "\n",
    "<small>Note: The `capture` *magic* is another way we could handle output.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%capture hadoop_out\n",
    "# Execute MrJob on Hadoop and let the magic handle outputs\n",
    "! python $myscript $myinput -r hadoop 2> $mylog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bye\"\t1\r\n",
      "\"goodbye\"\t1\r\n",
      "\"hadoop\"\t2\r\n",
      "\"hello\"\t2\r\n",
      "\"world\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "hadoop_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected option hdfs_tmp_dir\r\n",
      "no configs found; falling back on auto-configuration\r\n",
      "no configs found; falling back on auto-configuration\r\n",
      "creating tmp directory /tmp/wordcount.root.20151013.130840.424615\r\n",
      "writing wrapper script to /tmp/wordcount.root.20151013.130840.424615/setup-wrapper.sh\r\n",
      "Using Hadoop version 2.6.0\r\n",
      "Copying local files into hdfs:///user/root/tmp/mrjob/wordcount.root.20151013.130840.424615/files/\r\n",
      "HADOOP: packageJobJar: [/tmp/hadoop-unjar6938543500754848653/] [] /tmp/streamjob3313232432800080753.jar tmpDir=null\r",
      "\r\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\r",
      "\r\n",
      "HADOOP: Connecting to ResourceManager at /0.0.0.0:8032\r",
      "\r\n",
      "HADOOP: Total input paths to process : 1\r",
      "\r\n",
      "HADOOP: number of splits:2\r",
      "\r\n",
      "HADOOP: Submitting tokens for job: job_1444732540385_0005\r",
      "\r\n",
      "HADOOP: Submitted application application_1444732540385_0005\r",
      "\r\n",
      "HADOOP: The url to track the job: http://ipyhadoop:8088/proxy/application_1444732540385_0005/\r",
      "\r\n",
      "HADOOP: Running job: job_1444732540385_0005\r",
      "\r\n",
      "HADOOP: Job job_1444732540385_0005 running in uber mode : false\r",
      "\r\n",
      "HADOOP:  map 0% reduce 0%\r",
      "\r\n",
      "HADOOP:  map 100% reduce 0%\r",
      "\r\n",
      "HADOOP:  map 100% reduce 100%\r",
      "\r\n",
      "HADOOP: Job job_1444732540385_0005 completed successfully\r",
      "\r\n",
      "HADOOP: Counters: 49\r",
      "\r\n",
      "HADOOP: \tFile System Counters\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of bytes read=104\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of bytes written=333792\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of read operations=0\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of large read operations=0\r",
      "\r\n",
      "HADOOP: \t\tFILE: Number of write operations=0\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of bytes read=374\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of bytes written=51\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of read operations=9\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of large read operations=0\r",
      "\r\n",
      "HADOOP: \t\tHDFS: Number of write operations=2\r",
      "\r\n",
      "HADOOP: \tJob Counters \r",
      "\r\n",
      "HADOOP: \t\tLaunched map tasks=2\r",
      "\r\n",
      "HADOOP: \t\tLaunched reduce tasks=1\r",
      "\r\n",
      "HADOOP: \t\tData-local map tasks=2\r",
      "\r\n",
      "HADOOP: \t\tTotal time spent by all maps in occupied slots (ms)=9369\r",
      "\r\n",
      "HADOOP: \t\tTotal time spent by all reduces in occupied slots (ms)=3376\r",
      "\r\n",
      "HADOOP: \t\tTotal time spent by all map tasks (ms)=9369\r",
      "\r\n",
      "HADOOP: \t\tTotal time spent by all reduce tasks (ms)=3376\r",
      "\r\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all map tasks=9369\r",
      "\r\n",
      "HADOOP: \t\tTotal vcore-seconds taken by all reduce tasks=3376\r",
      "\r\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all map tasks=9593856\r",
      "\r\n",
      "HADOOP: \t\tTotal megabyte-seconds taken by all reduce tasks=3457024\r",
      "\r\n",
      "HADOOP: \tMap-Reduce Framework\r",
      "\r\n",
      "HADOOP: \t\tMap input records=2\r",
      "\r\n",
      "HADOOP: \t\tMap output records=8\r",
      "\r\n",
      "HADOOP: \t\tMap output bytes=82\r",
      "\r\n",
      "HADOOP: \t\tMap output materialized bytes=110\r",
      "\r\n",
      "HADOOP: \t\tInput split bytes=300\r",
      "\r\n",
      "HADOOP: \t\tCombine input records=0\r",
      "\r\n",
      "HADOOP: \t\tCombine output records=0\r",
      "\r\n",
      "HADOOP: \t\tReduce input groups=5\r",
      "\r\n",
      "HADOOP: \t\tReduce shuffle bytes=110\r",
      "\r\n",
      "HADOOP: \t\tReduce input records=8\r",
      "\r\n",
      "HADOOP: \t\tReduce output records=5\r",
      "\r\n",
      "HADOOP: \t\tSpilled Records=16\r",
      "\r\n",
      "HADOOP: \t\tShuffled Maps =2\r",
      "\r\n",
      "HADOOP: \t\tFailed Shuffles=0\r",
      "\r\n",
      "HADOOP: \t\tMerged Map outputs=2\r",
      "\r\n",
      "HADOOP: \t\tGC time elapsed (ms)=226\r",
      "\r\n",
      "HADOOP: \t\tCPU time spent (ms)=2390\r",
      "\r\n",
      "HADOOP: \t\tPhysical memory (bytes) snapshot=517996544\r",
      "\r\n",
      "HADOOP: \t\tVirtual memory (bytes) snapshot=2138722304\r",
      "\r\n",
      "HADOOP: \t\tTotal committed heap usage (bytes)=280240128\r",
      "\r\n",
      "HADOOP: \tShuffle Errors\r",
      "\r\n",
      "HADOOP: \t\tBAD_ID=0\r",
      "\r\n",
      "HADOOP: \t\tCONNECTION=0\r",
      "\r\n",
      "HADOOP: \t\tIO_ERROR=0\r",
      "\r\n",
      "HADOOP: \t\tWRONG_LENGTH=0\r",
      "\r\n",
      "HADOOP: \t\tWRONG_MAP=0\r",
      "\r\n",
      "HADOOP: \t\tWRONG_REDUCE=0\r",
      "\r\n",
      "HADOOP: \tFile Input Format Counters \r",
      "\r\n",
      "HADOOP: \t\tBytes Read=74\r",
      "\r\n",
      "HADOOP: \tFile Output Format Counters \r",
      "\r\n",
      "HADOOP: \t\tBytes Written=51\r",
      "\r\n",
      "HADOOP: Output directory: hdfs:///user/root/tmp/mrjob/wordcount.root.20151013.130840.424615/output\r",
      "\r\n",
      "looking for job logs in /usr/local/hadoop/logs/history/\r\n",
      "Counters from step 1:\r\n",
      "  (no counters found)\r\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/wordcount.root.20151013.130840.424615/output\r\n",
      "removing tmp directory /tmp/wordcount.root.20151013.130840.424615\r\n",
      "deleting hdfs:///user/root/tmp/mrjob/wordcount.root.20151013.130840.424615 from HDFS\r\n"
     ]
    }
   ],
   "source": [
    "%cat $mylog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An example of creating notebook extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A pattern recurs often:\n",
    "\n",
    "* You have to write a file\n",
    "* You have to execute mrjob \n",
    "    - with or without hadoop parameter\n",
    "* You have to bring back the output\n",
    "\n",
    "Can we do this with an ipython magic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A MrJob dedicated extension\n",
    "\n",
    "Since there is none, \n",
    "we created an extension for helping users with mapreduce in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the extension\n",
    "%reload_ext mrjobmagic\n",
    "# note: reload_ext does not get error if you try two times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it works:\n",
    "\n",
    "* Cell extension\n",
    "\n",
    "```\n",
    "%%mapreduce LOCAL_INPUT [inline,hadoop]\n",
    "\n",
    "MAPPER function\n",
    "[COMBINER function]\n",
    "REDUCER function\n",
    "```\n",
    "\n",
    "* Line extension\n",
    "\n",
    "```\n",
    "%mapreduce LOCAL_INPUT MR_FILE [inline,hadoop]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mapreduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile = '/data/worker/books/prince.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file is /data/worker/books/prince.txt\n",
      "Saving jobs/script_000090887.py\n",
      "Executing python3 jobs/script_000090887.py -r inline /data/worker/books/prince.txt\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /tmp/script_000090887.root.20151013.131851.841947\n",
      "writing to /tmp/script_000090887.root.20151013.131851.841947/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /tmp/script_000090887.root.20151013.131851.841947/step-0-mapper-sorted\n",
      "> sort /tmp/script_000090887.root.20151013.131851.841947/step-0-mapper_part-00000\n",
      "writing to /tmp/script_000090887.root.20151013.131851.841947/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /tmp/script_000090887.root.20151013.131851.841947/step-0-reducer_part-00000 -> /tmp/script_000090887.root.20151013.131851.841947/output/part-00000\n",
      "Streaming final output from /tmp/script_000090887.root.20151013.131851.841947/output\n",
      "removing tmp directory /tmp/script_000090887.root.20151013.131851.841947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'jobs/script_000090887.py'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%mapreduce $myfile\n",
    "\n",
    "def mapper(self, _, line):\n",
    "    pass\n",
    "def reducer(self, key, line):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing on the existing Hadoop cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%mapreduce $file hadoop\n",
    "\n",
    "def mapper(self, _, line):\n",
    "    pass\n",
    "def reducer(self, key, line):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if i wanto to modify directly the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Line execution, give a file you created or modified \n",
    "%mapreduce $file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More than a single step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mrjob can be configured to run different steps\n",
    "\n",
    "for each step you can specify which part has to be executed\n",
    "and the method to use within the class you wrote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def steps(self):\n",
    "    return [\n",
    "        MRStep(\n",
    "            mapper=self.mapper_get_words,\n",
    "            combiner=self.combiner_count_words,\n",
    "            reducer=self.reducer_count_words),\n",
    "        MRStep(reducer=self.reducer_find_max_word)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A quick note\n",
    "\n",
    "* With MrJob you cannot connect to a **remote** Hadoop cluster. \n",
    "    - Hadoop does not allow job submissions (class or executables) from outside.\n",
    "* On the contrary EMR on Amazon can be accessible from your laptop.\n",
    "    - Amazon created the [boto api](http://boto.readthedocs.org/en/latest/ref/emr.html) to solve the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap with Mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should really read the [documentation of the latest version](http://mrjob.readthedocs.org/en/latest/).\n",
    "\n",
    "It covers every need without getting too much complicated.\n",
    "There are many other options and advanced behaviours to discover.\n",
    "\n",
    "<small>\n",
    "Note 1: We are using the most recent version (*release* **v.0.5.0dev** ) because its the first one to support Python 3.\n",
    "\n",
    "Note 2: Developer are there to help you, see my case in https://github.com/Yelp/mrjob/issues/1142\n",
    "</small>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* More documentation than any other framework or library\n",
    "* Write code in a single class (per Hadoop job)\n",
    "    * Map and Reduce are single methods\n",
    "    * Very clean and simple\n",
    "* Advanced configuration\n",
    "    * Configure multiple steps\n",
    "    * Handle command line options inside the python code (see docs)\n",
    "* Easily wrap input/output \n",
    "    * No data copy required with HDFS\n",
    "* Hadoop logs or errors directly into the script output\n",
    "* **Switch environment without changing the code...!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cons**\n",
    "\n",
    "* Doesn’t give you the same level of access to Hadoop APIs \n",
    "    - Better: Dumbo and Pydoop\n",
    "    - Other libraries can be faster if you use typedbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison\n",
    "<img src='http://blog.cloudera.com/wp-content/uploads/2013/01/features.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance\n",
    "<img src='http://blog.cloudera.com/wp-content/uploads/2013/01/performance.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "source: http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of Chapter"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
