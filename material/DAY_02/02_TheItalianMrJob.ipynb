{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A more pythonic MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With Hadoop Streaming we switched MapReduce from Java to Python.\n",
    "\n",
    "How could we improve some more? \n",
    "\n",
    "What problems do we deal with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have to directly deal with HDFS\n",
    "* move input files\n",
    "* recover outputs\n",
    "* human make mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not easy debugging\n",
    "\n",
    "* logs via jobtracker\n",
    "* errors are Java stacktrace, often unrelated with the real problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have to write two different files\n",
    "* one for the mapper\n",
    "* one for the reducer\n",
    "* not a **module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we shape the Map Reduce job into Python syntax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MapReduce():\n",
    "    \n",
    "    def mapper(self, line):\n",
    "        pass\n",
    "    def reducer(self, sorted_line):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MRJob \n",
    "A more pythonic MapReduce library from Yelp\n",
    "\n",
    "<img src='https://avatars1.githubusercontent.com/u/49071?v=3&s=400' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> “Easiest route to Python programs that run on Hadoop”\n",
    "\n",
    "Install with: \n",
    "```bash\n",
    "pip install mrjob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code locally without installing Hadoop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "...or run it on a cluster of your choice!\n",
    "\n",
    "The same code may run locally, on Hadoop, or Amazon **Elastic MapReduce** (EMR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does MrJob work?\n",
    "\n",
    "* Python module built **on top of Hadoop Streaming**\n",
    "* Wrap HDFS pre and post processing if hadoop exists\n",
    "* automatically serializes/deserializes data flow out of each task \n",
    "    - e.g. JSON: json.loads() and json.dumps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A job is defined by a class extended from MRJob package\n",
    "\n",
    "* Contains methods that define the steps of a Hadoop job\n",
    "* A “step” consists of a mapper, a combiner, and a reducer. \n",
    "* All of those  are optional, though you must have at least one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class myjob(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        pass\n",
    "    def combiner(self, key, values):\n",
    "        pass\n",
    "    def reducer(self, key, values):\n",
    "        pass\n",
    "    def steps(self):\n",
    "        return [ MRStep(mapper=self.mapper, combiner=self.combiner, reducer=self.reducer) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mapper\n",
    "\n",
    "The mapper() method takes a key and a value as args\n",
    "\n",
    "```\n",
    "    def mapper(self, _, line):\n",
    "        pass\n",
    "```\n",
    "\n",
    "* E.g. key is ignored and a single line of text input is the value\n",
    "\n",
    "* Yields as many key-value pairs as it likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reducer\n",
    "\n",
    "The reduce() method takes a key and an iterator of values\n",
    "\n",
    "```\n",
    "    def reducer(self, key, values):\n",
    "        pass\n",
    "```\n",
    "\n",
    "* Also yields as many key-value pairs as it likes\n",
    "    * E.g. it sums the values for each key\n",
    "* Represent the  numbers of characters, words, and lines in the initial input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mymrjob/wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "class MRWordCount(MRJob):\n",
    "    \"\"\" Wordcount with MapReduce in a pythonic way\"\"\"\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split():\n",
    "             yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note!\n",
    "    \n",
    "Thanks to MrJob and generators we do not care inside the reducer to check when the value is changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## I/O\n",
    "\n",
    "You can pass input via stdin but be aware that mrjob will just dump it to a file first:\n",
    "```bash\n",
    "$ python my_job.py < input.txt\n",
    "```\n",
    "\n",
    "You can pass multiple input files, mixed with stdin (using the – character)\n",
    "```bash\n",
    "$ python my_job.py input1.txt input2.txt - < input3.txt\n",
    "```\n",
    "\n",
    "By default, output will be written to stdout.\n",
    "```bash\n",
    "$ python my_job.py input.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How you should execute in Batch Mode\n",
    "# Note: MrJob cannot run interactively inside a cell\n",
    "\n",
    "! python MYPYTHONFILE.py MYINPUT.txt 1> OUTPUT.txt 2> STDERR.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's an empty **template** to work with in copy/paste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" MapReduce easily with Python \"\"\"\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class job(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        pass\n",
    "    def reducer(self, key, line):\n",
    "        pass\n",
    "    def steps(self):\n",
    "        return [ \n",
    "            MRStep(mapper=self.mapper, reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    job.run()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With MRStep you define iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Count the different vowels inside the divine comedy using a MrJob class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running on Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By default, mrjob will run your job on your local (normal) environment) in a single Python process \n",
    "\n",
    "You change the way the job is run with the `-r/--runner` option:\n",
    "\n",
    "```\n",
    "-r inline, -r local, -r hadoop, or -r emr\n",
    "```\n",
    "\n",
    "Use also `--verbose` option to show all the steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we just need to add `-r hadoop`.\n",
    "\n",
    "<small>Note: The `capture` *magic* is another way we could handle output.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%capture hadoop_out\n",
    "# Execute MrJob on Hadoop and let the magic handle outputs\n",
    "! python $myscript $myinput -r hadoop 2> $mylog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bye\"\t1\r\n",
      "\"goodbye\"\t1\r\n",
      "\"hadoop\"\t2\r\n",
      "\"hello\"\t2\r\n",
      "\"world\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "hadoop_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /etc/mrjob.conf\r\n",
      "Using Hadoop version 2.6.0\r\n",
      "Copying local files to hdfs:///user/jovyan/tmp/mrjob/wordcount.jovyan.20160315.231008.131608/files/...\r\n",
      "Running step 1 of 1...\r\n",
      "  packageJobJar: [/tmp/hadoop-unjar8386463698946503097/] [] /tmp/streamjob581799815579415374.jar tmpDir=null\r\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\r\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\r\n",
      "  Total input paths to process : 1\r\n",
      "  number of splits:2\r\n",
      "  Submitting tokens for job: job_1458083121911_0001\r\n",
      "  Submitted application application_1458083121911_0001\r\n",
      "  The url to track the job: http://76752dc90450:8088/proxy/application_1458083121911_0001/\r\n",
      "  Running job: job_1458083121911_0001\r\n",
      "  Job job_1458083121911_0001 running in uber mode : false\r\n",
      "   map 0% reduce 0%\r\n",
      "   map 100% reduce 0%\r\n",
      "   map 100% reduce 100%\r\n",
      "  Job job_1458083121911_0001 completed successfully\r\n",
      "  Output directory: hdfs:///user/jovyan/tmp/mrjob/wordcount.jovyan.20160315.231008.131608/output\r\n",
      "Counters: 49\r\n",
      "\tFile Input Format Counters \r\n",
      "\t\tBytes Read=74\r\n",
      "\tFile Output Format Counters \r\n",
      "\t\tBytes Written=51\r\n",
      "\tFile System Counters\r\n",
      "\t\tFILE: Number of bytes read=104\r\n",
      "\t\tFILE: Number of bytes written=329064\r\n",
      "\t\tFILE: Number of large read operations=0\r\n",
      "\t\tFILE: Number of read operations=0\r\n",
      "\t\tFILE: Number of write operations=0\r\n",
      "\t\tHDFS: Number of bytes read=388\r\n",
      "\t\tHDFS: Number of bytes written=51\r\n",
      "\t\tHDFS: Number of large read operations=0\r\n",
      "\t\tHDFS: Number of read operations=9\r\n",
      "\t\tHDFS: Number of write operations=2\r\n",
      "\tJob Counters \r\n",
      "\t\tData-local map tasks=2\r\n",
      "\t\tLaunched map tasks=2\r\n",
      "\t\tLaunched reduce tasks=1\r\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=12473344\r\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=4098048\r\n",
      "\t\tTotal time spent by all map tasks (ms)=12181\r\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=12181\r\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4002\r\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4002\r\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12181\r\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4002\r\n",
      "\tMap-Reduce Framework\r\n",
      "\t\tCPU time spent (ms)=2130\r\n",
      "\t\tCombine input records=0\r\n",
      "\t\tCombine output records=0\r\n",
      "\t\tFailed Shuffles=0\r\n",
      "\t\tGC time elapsed (ms)=166\r\n",
      "\t\tInput split bytes=314\r\n",
      "\t\tMap input records=2\r\n",
      "\t\tMap output bytes=82\r\n",
      "\t\tMap output materialized bytes=110\r\n",
      "\t\tMap output records=8\r\n",
      "\t\tMerged Map outputs=2\r\n",
      "\t\tPhysical memory (bytes) snapshot=889561088\r\n",
      "\t\tReduce input groups=5\r\n",
      "\t\tReduce input records=8\r\n",
      "\t\tReduce output records=5\r\n",
      "\t\tReduce shuffle bytes=110\r\n",
      "\t\tShuffled Maps =2\r\n",
      "\t\tSpilled Records=16\r\n",
      "\t\tTotal committed heap usage (bytes)=530579456\r\n",
      "\t\tVirtual memory (bytes) snapshot=2645000192\r\n",
      "\tShuffle Errors\r\n",
      "\t\tBAD_ID=0\r\n",
      "\t\tCONNECTION=0\r\n",
      "\t\tIO_ERROR=0\r\n",
      "\t\tWRONG_LENGTH=0\r\n",
      "\t\tWRONG_MAP=0\r\n",
      "\t\tWRONG_REDUCE=0\r\n",
      "Streaming final output from hdfs:///user/jovyan/tmp/mrjob/wordcount.jovyan.20160315.231008.131608/output...\r\n",
      "Removing HDFS temp directory hdfs:///user/jovyan/tmp/mrjob/wordcount.jovyan.20160315.231008.131608...\r\n"
     ]
    }
   ],
   "source": [
    "%cat $mylog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MRJob add many comodities!\n",
    "\n",
    "Some of them may result expensive on heavy computing.\n",
    "\n",
    "For example MapReduce data transfer is serialize in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class MyMRJob(mrjob.job.MRJob):\n",
    "\n",
    "    # these are the defaults\n",
    "    INPUT_PROTOCOL = mrjob.protocol.RawValueProtocol\n",
    "    INTERNAL_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    OUTPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see what it means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing protocol.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile protocol.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "\n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split(' '):\n",
    "             yield word.lower(), 1\n",
    "\n",
    "                \n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bye\"\t1\r\n",
      "\"goodbye\"\t1\r\n",
      "\"hadoop\"\t2\r\n",
      "\"hello\"\t2\r\n",
      "\"world\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "! python protocol.py /data/lectures/data/books/twolines.txt 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting protocol.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile protocol.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import PickleProtocol\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "\n",
    "    # Optimization on internal protocols\n",
    "    INTERNAL_PROTOCOL = PickleProtocol\n",
    "    OUTPUT_PROTOCOL = PickleProtocol\n",
    "    \n",
    "    def mapper(self, key, line):\n",
    "        for word in line.split(' '):\n",
    "             yield word.lower(), 1\n",
    "\n",
    "                \n",
    "    def reducer(self, word, occurrences):\n",
    "        yield word, sum(occurrences)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\x80\\x03X\\x03\\x00\\x00\\x00byeq\\x00.\t\\x80\\x03K\\x01.\r\n",
      "\\x80\\x03X\\x05\\x00\\x00\\x00helloq\\x00.\t\\x80\\x03K\\x02.\r\n",
      "\\x80\\x03X\\x05\\x00\\x00\\x00worldq\\x00.\t\\x80\\x03K\\x02.\r\n",
      "\\x80\\x03X\\x06\\x00\\x00\\x00hadoopq\\x00.\t\\x80\\x03K\\x02.\r\n",
      "\\x80\\x03X\\x07\\x00\\x00\\x00goodbyeq\\x00.\t\\x80\\x03K\\x01.\r\n"
     ]
    }
   ],
   "source": [
    "! python protocol.py /data/lectures/data/books/twolines.txt 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Runners\n",
    "\n",
    "http://mrjob.readthedocs.org/en/latest/guides/runners.html\n",
    "\n",
    "You cannot use the programmatic runner functionality in the same file as your job class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## RUNNER: JUST AN EXAMPLE\n",
    "\n",
    "# Load class for mapreduce\n",
    "from mypackage import SomeClassWeWrote\n",
    "#from job import MRcoverage\n",
    "from mrjob.util import log_to_stream\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Create object\n",
    "    mrjob = SomeClassWeWrote(args=[   \\\n",
    "        '-r', 'inline',\n",
    "        #'-r', 'local',\n",
    "        # '-r', 'hadoop',\n",
    "        # '--jobconf=mapreduce.job.maps=10',\n",
    "        # '--jobconf=mapreduce.job.reduces=4'\n",
    "        #'--jobconf=stream.recordreader.compression=bz2'\n",
    "        ])\n",
    "\n",
    "    # Run and handle output\n",
    "    with mrjob.make_runner() as runner:\n",
    "\n",
    "        # Redirect hadoop logs to stderr\n",
    "        log_to_stream(\"mrjob\")\n",
    "        # Execute the job\n",
    "        runner.run()\n",
    "\n",
    "        # Do something with stream output (e.g. file, database, etc.)\n",
    "        for line in runner.stream_output():\n",
    "            key, value = mrjob.parse_output_line(line)\n",
    "            print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap with Mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You should really read the [documentation of the latest version](http://mrjob.readthedocs.org/en/latest/).\n",
    "\n",
    "It covers every need without getting too much complicated.\n",
    "There are many other options and advanced behaviours to discover.\n",
    "\n",
    "<small>\n",
    "Note 1: We are using the most recent version (*release* **v.0.5.0dev** ) because its the first one to support Python 3.\n",
    "\n",
    "Note 2: Developer are there to help you, see my case in https://github.com/Yelp/mrjob/issues/1142\n",
    "</small>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* More documentation than any other framework or library\n",
    "* Write code in a single class (per Hadoop job)\n",
    "    * Map and Reduce are single methods\n",
    "    * Very clean and simple\n",
    "* Advanced configuration\n",
    "    * Configure multiple steps\n",
    "    * Handle command line options inside the python code (see docs)\n",
    "* Easily wrap input/output \n",
    "    * No data copy required with HDFS\n",
    "* Hadoop logs or errors directly into the script output\n",
    "* **Switch environment without changing the code...!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cons**\n",
    "\n",
    "* Doesn’t give you the same level of access to Hadoop APIs \n",
    "    - Better: Dumbo and Pydoop\n",
    "    - Other libraries can be faster if you use typedbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison\n",
    "<img src='http://blog.cloudera.com/wp-content/uploads/2013/01/features.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance\n",
    "<img src='http://blog.cloudera.com/wp-content/uploads/2013/01/performance.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "source: http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One final note*: \n",
    "> Open source is a great thing\n",
    "\n",
    "The community listens to you:\n",
    "https://github.com/Yelp/mrjob/issues/1142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'watermark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f43abc964800>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load_ext watermark'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'watermark -a \"Paolo D.\" -d -v -m -p jupyter'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-64>\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.5/site-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing module name.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'already loaded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.5/site-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_load_ipython_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'watermark'"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Paolo D.\" -d -v -m -p jupyter"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
