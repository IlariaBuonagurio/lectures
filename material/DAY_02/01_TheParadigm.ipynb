{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MapReduce\n",
    "\n",
    "A different paradigm for processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**MapReduce** is a completely different paradigm\n",
    "\n",
    "\n",
    "* Solving a certain subset of parallelizable problems\n",
    "* It brings the compute to the data\\n\",\n",
    "* Input data is not stored on a separate storage system\n",
    "    * Data exists in little pieces and is permanently stored on each computing node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What does it mean by “sharding” a database?\n",
    "Partition the database into smaller parts by using a column-based selection (vertical partitioning)\n",
    "Partition the database into smaller parts by using a row-based selection (horizontal partitioning)\n",
    "Partition the database into smaller parts by using a standard hash function: k(x) -> n(x * p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAP theorema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MapReduce is the programming paradigm that allows massive scalability across thousands of servers.\n",
    "\n",
    "Its open source server implementation is the *Hadoop* cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop\n",
    "(and Hadoop streaming)\n",
    "\n",
    "<center>\n",
    "<img src='http://www.opensourceforu.efytimes.com/wp-content/uploads/2012/03/hadoop-database-590x321.jpg'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Data replication\n",
    "\n",
    "What’s the reason why a new data set put onto a HDFS volume gets replicated three times of different nodes?\n",
    "\n",
    "Performance. It improves the data locality.\n",
    "Fault-tolerance. If a node crashes, its load can be moved onto another resource\n",
    "Performance and fault-tolerance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Also always keep in mind that ***HDFS*** is fundamental to Hadoop \n",
    "\n",
    "* it provides the data chunking distribution across compute elements \n",
    "* necessary for map/reduce applications to be efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word count\n",
    "The '`Hello World`' for MapReduce (with *Java*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Among the simplest of full Hadoop jobs you can run\n",
    "\n",
    "<img src='http://www.glennklockwood.com/data-intensive/hadoop/wordcount-schematic.png'\n",
    "width='700'>\n",
    "<small>Reading ***Moby Dick*** </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider doing a word count of the following file using  MapReduce:\n",
    "```\n",
    "Hello World Bye World\n",
    "Hello Hadoop Goodbye Hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The map function reads in words one at a time outputs (“word”, 1) for each parsed input word\n",
    "\n",
    "```\n",
    "(Hello, 1)\n",
    "(World, 1)\n",
    "(Bye, 1)\n",
    "(World, 1)\n",
    "(Hello, 1)\n",
    "(Hadoop, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The shuffle phase between map and reduce creates a  list of values associated with each key\n",
    "```\n",
    "(Bye, (1))\n",
    "(Goodbye, (1))\n",
    "(Hadoop, (1, 1))\n",
    "(Hello, (1, 1))\n",
    "(World, (1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce function sums the numbers in the list for each  key and outputs (word, count) pairs\n",
    "```\n",
    "(Bye, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 2)\n",
    "(Hello, 2)\n",
    "(World, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How can you do this with Java?\n",
    "(the Hadoop framework native language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "// Imports\n",
    "package org.myorg;\n",
    "import java.io.IOException;\n",
    "import java.util.*;\n",
    "import org.apache.hadoop.*\n",
    "\n",
    "// Create JAVA class\n",
    "public class WordCount {\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Mapper function\n",
    "  public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      String line = value.toString();\n",
    "      StringTokenizer tokenizer = new StringTokenizer(line);\n",
    "      while (tokenizer.hasMoreTokens()) {\n",
    "        word.set(tokenizer.nextToken());\n",
    "        output.collect(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Reducer function\n",
    "  public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      int sum = 0;\n",
    "      while (values.hasNext()) {\n",
    "        sum += values.next().get();\n",
    "      }\n",
    "      output.collect(key, new IntWritable(sum));\n",
    "    }\n",
    "  }\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "``` Java\n",
    "//Main function\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    JobConf conf = new JobConf(WordCount.class);\n",
    "    conf.setJobName(\"wordcount\");\n",
    "\n",
    "    conf.setOutputKeyClass(Text.class);\n",
    "    conf.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    conf.setMapperClass(Map.class);\n",
    "    conf.setCombinerClass(Reduce.class);\n",
    "    conf.setReducerClass(Reduce.class);\n",
    "\n",
    "    conf.setInputFormat(TextInputFormat.class);\n",
    "    conf.setOutputFormat(TextOutputFormat.class);\n",
    "\n",
    "    FileInputFormat.setInputPaths(conf, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(conf, new Path(args[1]));\n",
    "\n",
    "    JobClient.runJob(conf);\n",
    "  }\n",
    "```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can test the Java code here. *Live*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_EXAMPLES=/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar\n"
     ]
    }
   ],
   "source": [
    "%env HADOOP_EXAMPLES /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\r\n",
      "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\r\n",
      "  multifilewc: A job that counts words from several files.\r\n",
      "  wordcount: A map/reduce program that counts the words in the input files.\r\n",
      "  wordmean: A map/reduce program that counts the average length of the words in the input files.\r\n",
      "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\r\n",
      "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\r\n"
     ]
    }
   ],
   "source": [
    "# Hadoop available examples\n",
    "! hadoop jar $HADOOP_EXAMPLES | grep word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wordcount <in> [<in>...] <out>\r\n"
     ]
    }
   ],
   "source": [
    "# Check wordcount\n",
    "! hadoop jar $HADOOP_EXAMPLES wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Demo\n",
    "\n",
    "Note to my self: run next cells 'live'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_EXAMPLES=/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar\n"
     ]
    }
   ],
   "source": [
    "%env HADOOP_EXAMPLES /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World Bye World\r\n",
      "Hello Hadoop Goodbye Hadoop"
     ]
    }
   ],
   "source": [
    "# Our input\n",
    "! cat ../data/books/twolines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "########################\n",
    "# Preprocess with HDFS\n",
    "\n",
    "# Create input directory\n",
    "hdfs dfs -mkdir myinput\n",
    "# Save one file inside\n",
    "file=\"../data/books/twolines.txt\"\n",
    "hdfs dfs -put $file myinput/file01\n",
    "# Remove output or Hadoop will give error if existing\n",
    "hdfs dfs -rm -r -f myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/04/13 13:05:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/04/13 13:05:45 INFO input.FileInputFormat: Total input paths to process : 1\n",
      "16/04/13 13:05:45 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/04/13 13:05:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1460551715305_0001\n",
      "16/04/13 13:05:46 INFO impl.YarnClientImpl: Submitted application application_1460551715305_0001\n",
      "16/04/13 13:05:46 INFO mapreduce.Job: The url to track the job: http://mynotebook:8088/proxy/application_1460551715305_0001/\n",
      "16/04/13 13:05:46 INFO mapreduce.Job: Running job: job_1460551715305_0001\n",
      "16/04/13 13:05:58 INFO mapreduce.Job: Job job_1460551715305_0001 running in uber mode : false\n",
      "16/04/13 13:05:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/04/13 13:06:06 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/04/13 13:06:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/04/13 13:06:15 INFO mapreduce.Job: Job job_1460551715305_0001 completed successfully\n",
      "16/04/13 13:06:15 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=67\n",
      "\t\tFILE: Number of bytes written=212879\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=163\n",
      "\t\tHDFS: Number of bytes written=41\n",
      "\t\tHDFS: Number of read operations=6\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4960\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5940\n",
      "\t\tTotal time spent by all map tasks (ms)=4960\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5940\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4960\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5940\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5079040\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6082560\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=82\n",
      "\t\tMap output materialized bytes=67\n",
      "\t\tInput split bytes=114\n",
      "\t\tCombine input records=8\n",
      "\t\tCombine output records=5\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=67\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=10\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=170\n",
      "\t\tCPU time spent (ms)=1260\n",
      "\t\tPhysical memory (bytes) snapshot=423378944\n",
      "\t\tVirtual memory (bytes) snapshot=1684758528\n",
      "\t\tTotal committed heap usage (bytes)=168497152\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=41\n"
     ]
    }
   ],
   "source": [
    "# Test wordcount with real hadoop on our system\n",
    "! hadoop jar $HADOOP_EXAMPLES wordcount myinput myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye\t1\r\n",
      "Goodbye\t1\r\n",
      "Hadoop\t2\r\n",
      "Hello\t2\r\n",
      "World\t2\r\n"
     ]
    }
   ],
   "source": [
    "# Hadoop output\n",
    "! hadoop fs -cat myoutput/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop streaming\n",
    "### Concepts and mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hadoop streaming is a utility \n",
    "\n",
    "* It comes bundled with the Hadoop distribution\n",
    "* It allows creating and running Map/Reduce jobs \n",
    "    - with any executable or script as the mapper and/or the reducer\n",
    "\n",
    "Protocol steps\n",
    "\n",
    "* Create a Map/Reduce job\n",
    "* Submit the job to an appropriate cluster\n",
    "* Monitor the progress of the job until it completes\n",
    "* Links to Hadoop HDFS job directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why?\n",
    "\n",
    "One of the most unappetizing aspects of Hadoop to users of traditional HPC is that it is written in Java. \n",
    "\n",
    "* Java is not originally designed to be a high-performance language\n",
    "* Learning Java is very difficult for domain scientists\n",
    "\n",
    "This is why Hadoop allows you to write map/reduce code in any language you want using the Hadoop Streaming interface\n",
    "\n",
    "* It means turning an existing Python or Perl script into a Hadoop job\n",
    "* Does not require learning any Java at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MapReduce streaming with binaries/executables\n",
    "\n",
    "* Executables are specified for mappers and reducers!\n",
    "    - each mapper task run as a separate process \n",
    "* Inputs converted into lines and feed to the `STDIN` of the process\n",
    "* The mapper collects `STDOUT` of the process \n",
    "    - each line is a key/value pair **separated by TAB**\n",
    "    - e.g. ”this is the key\\tvalue is the rest\\n”\n",
    "\n",
    "warning: If there is no tab character in the line, then entire line is considered as key and the value is null (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A streaming command line example **for python**:\n",
    "``` bash\n",
    "$ hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -files mapper.py,reducer.py\n",
    "    -input input_dir/ \\\n",
    "    -output output_dir/ \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before submitting the Hadoop Streaming job:\n",
    "\n",
    "* Make sure your scripts have no errors\n",
    "* Do mapper and reducer scripts actually work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is just a matter of running them through pipes on a **little bit** of sample data,\n",
    "\n",
    "like `cat` or `head` linux bash commands, with pipes, as seen before.\n",
    "\n",
    "```\n",
    "# Simulating hadoop streaming with bash pipes\n",
    "$ cat $file | python mapper.py | sort | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "    \n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "\n",
    "    mystop = mystart + len(myseq)\n",
    "\n",
    "    # Each element with coverage\n",
    "    for i in range(mystart,mystop):\n",
    "        results = [mychr+SEP+i.__str__(), \"1\"]\n",
    "        print(TAB.join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "last_value = \"\"\n",
    "value_count = 1\n",
    "for line in sys.stdin:\n",
    "    value, count = line.strip().split(TAB)\n",
    "    # if this is the first iteration\n",
    "    if not last_value:\n",
    "        last_value = value\n",
    "    # if they're the same, log it\n",
    "    if value == last_value:\n",
    "        value_count += int(count)\n",
    "    else:\n",
    "        # state change\n",
    "        try: \n",
    "            print(TAB.join([last_value, str(value_count)]))\n",
    "        except:\n",
    "            pass\n",
    "        last_value = value\n",
    "        value_count = 1\n",
    "# LAST ONE after all records have been received\n",
    "print(TAB.join([last_value, str(value_count)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1:10000\t3\n",
      "chr1:10001\t2\n",
      "chr1:10002\t2\n",
      "chr1:10003\t2\n",
      "chr1:10004\t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m3.880s\n",
      "user\t0m3.800s\n",
      "sys\t0m0.070s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# needs ~ 5 seconds for running\n",
    "time head -n 10000 $myfile | python mapper.py | sort | python reducer.py | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<big>\n",
    "A working python code tested on pipes **should work** with Hadoop Streaming\n",
    "</big>\n",
    "\n",
    "* To make this happen we need to handle copy of input and output files inside the Hadoop FS\n",
    "* Also the job tracker logs will be found inside HDFS\n",
    "* We are going to use bash scripting inside the notebook to make our workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HDFS commands to interact with Hadoop file system use the same syntax:\n",
    "\n",
    "```\n",
    "hdfs dfs -command\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`command` are like bash commands for file\n",
    "\n",
    "e.g.\n",
    "\n",
    "```\n",
    "hadoop fs -mkdir hdfs:///dir\n",
    "hadoop fs -put file_on_host hdfs:///path/to/file\n",
    "hadoop fs -ls\n",
    "```\n",
    "\n",
    "<small>\n",
    "Note: we have seen this in action with the Java example\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<big>\n",
    "Hadoop Streaming needs “binaries” to execute\n",
    "</big>\n",
    "\n",
    "You need to specify interpreter at the beginning of your scripts:\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "```\n",
    "\n",
    "Make also the script executables:\n",
    "```\n",
    "chmod +x hs*.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! chmod +x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HADOOP_STREAMING=/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar\n"
     ]
    }
   ],
   "source": [
    "%env HADOOP_STREAMING /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar [options]\n",
      "Options:\n",
      "  dumptb <glob-pattern> Dumps all files that match the given pattern to \n",
      "                        standard output as typed bytes.\n",
      "  loadtb <path> Reads typed bytes from standard input and stores them in\n",
      "                a sequence file in the specified path\n",
      "  [streamjob] <args> Runs streaming job with given arguments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No Arguments Given!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Launch streaming\n",
    "hadoop jar $HADOOP_STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted myinput\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/03/15 17:04:11 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Preprocess with HDFS\n",
    "hdfs dfs -rm -r -f myinput\n",
    "hdfs dfs -mkdir myinput\n",
    "# Save one file inside\n",
    "hdfs dfs -put /tmp/ngs.sam myinput/file01\n",
    "# Remove output or Hadoop will give error if existing\n",
    "hdfs dfs -rm -r -f myoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Final launch via bash command for using Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "# A real Hadoop Streaming run\n",
    "time hadoop jar $HADOOP_STREAMING \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -input myinput -output myoutput \\\n",
    "    -mapper mapper.py -reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hadoop streaming is **difficult to debug**.\n",
    "Just like real Java Hadoop.\n",
    "\n",
    "If you did a typical setup mistake, you may end receiving unrelated errors stacktrace from the Java virtual machine.\n",
    "\n",
    "So before googling those stacktrace, make sure that:\n",
    "\n",
    "* Python files (mapper and reducer) exists\n",
    "* They are provided inside the main bash command also as **files** list\n",
    "* They are executables and contain as first line the hashbang\n",
    "* Your input directory exists on HDFS\n",
    "* Files inside your input directory are not corrupted\n",
    "    - e.g. bad decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paolo D. 2016-04-12 \n",
      "\n",
      "CPython 3.5.1\n",
      "IPython 4.1.2\n",
      "\n",
      "jupyter 1.0.0\n",
      "\n",
      "compiler   : GCC 4.4.7 20120313 (Red Hat 4.4.7-1)\n",
      "system     : Linux\n",
      "release    : 4.3.3-dhyve\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 1\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Paolo D.\" -d -v -m -p jupyter"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
